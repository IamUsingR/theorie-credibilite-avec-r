%%% Copyright (C) 2018 Vincent Goulet
%%%
%%% Ce fichier fait partie du projet
%%% «Théorie de la crédibilité avec R»
%%% http://github.com/vigou3/theorie-credibilite-avec-r
%%%
%%% Cette création est mise à disposition selon le contrat
%%% Attribution-Partage dans les mêmes conditions 4.0
%%% International de Creative Commons.
%%% http://creativecommons.org/licenses/by-sa/4.0/

\chapter{Tarification bayésienne}
\label{chap:bayesienne}

<<echo=FALSE>>=
library(actuar)
options(width = 52)
@

Ce chapitre constitue notre porte d'entrée vers la crédibilité de
précision. Il est très important pour la suite puisque nous y poserons
les bases sur lesquelles nous construirons les modèles de crédibilité,
notamment le modèle d'hétérogénéité.

Avant d'aller plus loin, rappelons que l'objectif que nous poursuivons
consiste à établir une tarification optimale pour un regroupement (ou
portefeuille) hétérogène de contrats d'assurance. Par «optimale», nous
entendons que la tarification est la plus précise pour chaque contrat
du regroupement.

L'approche purement bayésienne explorée dans ce chapitre est plutôt
théorique. Vous constaterez que la notion de regroupement de contrats
y joue un rôle très mineur, dans la mesure où nous pouvons effectuer
toute notre modélisation sans vraiment y faire référence. Le
portefeuille devient incontournable dans lorsque nous devons estimer
certaines quantités à partir de données. Nous n'aborderons toutefois
cette approche empirique qu'au \autoref{chap:buhlmann}.

Le chapitre débute par une mise en situation très simple, mais qui
permet de bien illustrer le genre de problèmes auxquels nous sommes
confrontés en tarification basée sur l'expérience, ainsi que les
solutions que nous proposerons avec l'approche bayésienne. À la
\autoref{sec:bayesienne:modele}, nous mettrons en place un modèle
mathématique permettant de décrire l'état d'hétérogénéité du
portefeuille. De ce modèle découleront, à la
\autoref{sec:bayesienne:prevision}, diverses prévisions, pour employer
la terminologie statistique. En termes actuariels, ces prévisions
constituent des primes d'assurance. À l'aide de plusieurs exemples,
nous démontrerons que calcul de la prime bayésienne peut parfois
s'avérer ardu et nous établirons les conditions faisant en sorte que
cette prime est non seulement simple à calculer, mais qu'elle adopte
une forme linéaire facile à interpréter.

Si le contexte et la terminologie de l'approche bayésienne en
statistique ne vous sont pas familiers ou frais à la mémoire, révisez
le contenu de l'\autoref{chap:estimation-bayesienne} avant d'aller
plus loin.


\section{Mise en situation}
\label{sec:bayesienne:mise}

Un portefeuille d'assurance automobile est composé de $75$~\% de bons
conducteurs et de $25$~\% de mauvais conducteurs. Les bons conducteurs
ont une probabilité de $1/15$ d'avoir un accident, alors que la
probabilité est de $1/10$ pour les mauvais conducteurs. Le coût d'un
accident est de $\nombre{1000}$ et nous supposons qu'un conducteur ne
peut subir qu'un accident dans une année.

\begin{enumerate}
\item Nous choisissons un assuré au hasard. Quelle est la probabilité
  que cet assuré ait un accident dans l'année qui suit?

  Pour effectuer les calculs, nous définissons les deux événements
  suivants:
  \begin{align*}
    A &: \text{avoir un accident} \\
    B &: \text{être un bon conducteur}.
  \end{align*}
  Nous noterons $\bar{B}$ le fait d'être un mauvais conducteur.

  Nous cherchons $\Pr[A]$ sachant que
  \begin{align*}
    \Pr[A|B] &= \frac{1}{15} \\
    \Pr[A|\bar{B}] &= \frac{1}{10} \\
    \Pr[B] &= \frac{3}{4} \\
    \Pr[\bar{B}] &= \frac{1}{4}.
  \end{align*}
  Par la loi des probabilités totales,
  \begin{align*}
    \Pr[A]
    &= \Pr[A|B] \Pr[B] + \Pr[A|\bar{B}] \Pr[\bar{B}] \\
    &= \left( \frac{1}{15} \right) \left( \frac{3}{4} \right)
      + \left( \frac{1}{10} \right) \left( \frac{1}{4} \right) \\
    &= \frac{3}{40}.
  \end{align*}

  \tipbox{Nous pouvons aussi exprimer cette probabilité sous la forme
    \begin{equation*}
      \Pr[A] = \esp{\Pr[A|\text{type de conducteur}]},
    \end{equation*}
    ce qui permet d'accentuer le fait qu'il s'agit d'une moyenne
    (pondérée) des deux probabilités d'avoir un accident.}

\item Quelle est la prime pure de cet assuré la première année?

  Soit $S$ le montant total des sinistres de cet assuré. La prime pure
  correspond à la quantité $\esp{S}$. Pour calculer cette espérance,
  nous pouvons procéder de deux façons.

  Tout d'abord, dans la mesure où l'assuré ne peut avoir qu'un seul
  sinistre dans l'année, que le montant de celui-ci est
  $\nombre{1000}$ et que la probabilité que ce sinistre survienne est
  $3/40$, tel que calculé ci-dessus, la fonction de masse de
  probabilité de la variable aléatoire $S$ est:
  \begin{equation*}
    \Pr[S = x] =
    \begin{cases}
      \frac{3}{40}, & x = \nombre{1000} \\
      \frac{37}{40}, & x = 0.
    \end{cases}
  \end{equation*}
  Par conséquent,
  \begin{equation*}
    \esp{S} = \nombre{1000} \left( \frac{3}{40} \right) = 75.
  \end{equation*}

  L'autre approche pour calculer la prime pure d'un conducteur choisi
  au hasard consiste à calculer la prime pure pour chaque type
  de conducteur et à calculer la moyenne de ces primes. De l'énoncé de
  la mise en situation, nous pouvons établir
  \begin{align*}
    \Pr[S = x|B]
    &=
      \begin{cases}
        \frac{1}{15}, & x = \nombre{1000} \\
        \frac{14}{15}, & x = 0
      \end{cases} \\
    \Pr[S = x|\bar{B}]
    &=
      \begin{cases}
        \frac{1}{10}, & x = \nombre{1000} \\
        \frac{9}{10}, & x = 0,
      \end{cases}
  \end{align*}
  d'où
  \begin{align*}
    \esp{S|B} &= \frac{200}{3} \\
    \intertext{et}
    \esp{S|\bar{B}} &= 100.
  \end{align*}
  Ainsi, nous obtenons la même prime pure que ci-dessus:
    \begin{align*}
      \esp{S}
      &= \esp{\esp{S|\text{type de conducteur}}} \\
      &= \esp{S|B} \Pr[B] + \esp{S|\bar{B}} \Pr[\bar{B}] \\
      &= \frac{200}{3} \left( \frac{3}{4} \right)
        + 100 \left( \frac{1}{4} \right) \\
      &= 75.
    \end{align*}

\item L'assuré choisi précédemment a subi un accident dans la première
  année. Quelle est la probabilité qu'il s'agisse d'un bon conducteur?

  Nous cherchons $\Pr[B|A]$, que nous pouvons interpréter comme la
  révision de notre opinion sur le niveau de risque du conducteur
  suite à l'ajout d'information. Or, par la règle de Bayes,
  \begin{align*}
    \Pr[B|A]
    &= \frac{\Pr[A|B] \Pr[B]}{\Pr[A]} \\
    &= \frac{(1/15) (1/4)}{3/40} \\
    &= \frac{2}{3} < \frac{3}{4}.
  \end{align*}
  Nous estimons donc moins probable que l'assuré soit un bon
  conducteur puisqu'il a subi un accident dans la première année.

\item Quelle est la prime pure de cet assuré pour la seconde année?

  Nous cherchons $\esp{S|A}$. Encore ici, nous pouvons procéder de
  deux manières qui s'avèrent équivalentes en bout de ligne.

  La première approche consiste à calculer la fonction de masse de
  probabilité de la variable aléatoire $S|A$. Nous avons
  \begin{align*}
    \Pr[A|A]
    &= \Pr[A|B] \Pr[B|A] + \Pr[A|\bar{B}] \Pr[\bar{B}|A] \\
    &= \left( \frac{1}{15} \right) \left( \frac{2}{3} \right)
      + \left( \frac{1}{10} \right) \left( \frac{1}{3} \right) \\
    &= \frac{7}{90},
  \end{align*}
  d'où
  \begin{equation*}
    \Pr[S = x|A] =
    \begin{cases}
      \frac{7}{90}, & x = \nombre{1000} \\
      \frac{83}{90}, & x = 0
    \end{cases}
  \end{equation*}
  et
  \begin{equation*}
    \esp{S|A} = \nombre{1000} \left( \frac{7}{90} \right)
    = \frac{700}{9}.
  \end{equation*}

  Dans la seconde approche, nous calculons l'espérance en
  conditionnant sur le type de conducteur:
  \begin{align*}
    \esp{S|A}
    &= \esp{S|B} \Pr[B|A] + \esp{S|\bar{B}} \Pr[\bar{B}|A] \\
    &= \frac{200}{3} \left( \frac{2}{3} \right)
      + 100 \left( \frac{1}{3} \right) \\
    &= \frac{700}{9}.
  \end{align*}
\end{enumerate}



\section{Modèle d'hétérogénéité}
\label{sec:bayesienne:modele}

Cette section introduit le modèle que nous utiliserons dorénavant en
crédibilité de précision. Établi par
\citet{Buhlmann:credibility:1967,Buhlmann:credibility:1969}, ce modèle
permet de décrire remarquablement bien la situation à laquelle nous
sommes confrontés, à savoir que nous avons regroupé des contrats
d'assurance aux caractéristiques semblables, mais que subsiste dans le
groups de l'hétérogénéité dans les niveaux de risque. C'est cette
hétérogénéité qui exige que nous ayons recours à l'expérience des
contrats pour établir une tarification juste et équitable.

Ainsi, nous avons un portefeuille (groupe) hétérogène de $I$ contrats.
Le niveau de risque du contrat $i = 1, \dots, I$ est inconnu, mais des
données $S_{i1}, \dots, S_{in}$ sont disponibles pour fins de
tarification.

L'élément clé dans notre modèle est le suivant: nous posons
l'existence d'une variable aléatoire $\Theta_i$ qui représente le
\emph{niveau de risque} du contrat $i$. Cette variable aléatoire est
bien évidemment non observable --- il n'y aurait pas de problème de
tarification autrement --- et nous supposons qu'elle demeure constante
dans le temps. En d'autres termes, la variable aléatoire $\Theta_i$
constitue la synthèse de tous les facteurs de risque associé au contrat
$i$ que nous ne savons prendre en compte par ailleurs dans la
structure de classification.

La fonction de répartition de la variable aléatoire $\Theta$ --- aussi
appelée la \emph{fonction de structure} du portefeuille --- est
$U(\theta)$ et $u(\theta)$ est la fonction de densité (ou de masse) de
probabilité de correspondante.

Nous posons les hypothèses suivantes pour compléter notre modèle
d'hétérogénéité.
\begin{enumerate}
\item Les observations du contrat $i$ sont \emph{conditionnellement}
  indépendantes et identiquement distribuées avec fonction de
  répartition $F(x|\theta_i)$.
\item Les variables aléatoires $\Theta_1, \dots, \Theta_I$ sont
  identiquement distribuées avec fonction de répartition $U(\theta)$.
\item Les contrats sont indépendants.
\end{enumerate}

La troisième hypothèse est assez standard: elle établit que le dossier
d'un contrat n'a pas d'influence sur celui des autres contrats.

La portée des deux autres hypothèses est beaucoup plus grande. La
première dicte que les sinistres d'un contrat sont indépendants et
identiquement distribués seulement lorsque nous connaissons le niveau
de risque de ce contrat. Autrement, les sinistres sont dépendants.
C'est là un phénomène dit de \emph{contagion apparente}. La
\autoref{fig:bayesienne:contagion} illustre ce phénomène. Dans la
figure de gauche, les sinistres d'un contrat, représentés par des
points, sont tous supérieurs à la moyenne du groupe, représentée par
la ligne horizontale. C'est là une forme de dépendance. Pourtant,
lorsque la ligne représente plutôt la moyenne (normalement inconnue)
du contrat, nous constatons que les sinistres fluctuent simplement
aléatoirement autour de leur moyenne.

\begin{figure}
  \centering
  \begin{minipage}{0.48\linewidth}
<<echo=FALSE, fig=TRUE, width=5, height=5>>=
set.seed(1)
theta <- 5
x <- rpois(10, theta)
plot(x, pch = 19, ylim = c(0, 10),
     xlab = "t", ylab = expression(S[t]))
abline(h = 2, col = "orange")
@
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\linewidth}
<<echo=FALSE, fig=TRUE, width=5, height=5>>=
plot(x, pch = 19, ylim = c(0, 10),
     xlab = "t", ylab = expression(S[t]))
abline(h = theta, col = "orange")
@
  \end{minipage}
  \caption{Illustration du phénomène de contagion apparente. À gauche,
    la ligne horizontale représente la moyenne du groupe. À droite, la
    ligne représente le niveau de risque du contrat.}
  \label{fig:bayesienne:contagion}
\end{figure}

Les contrats du portefeuille sont différents en ce qu'ils possèdent
chacun leur propre niveau de risque. Cependant, en vertu de la seconde
hypothèse, ces niveaux de risque proviennent tous de la même
\emph{distribution}. Autrement dit, ils sont suffisamment similaires
pour justifier de les regrouper dans un même porfeuille.

\begin{exemple}
  Dans la mise en situation de la \autoref{sec:bayesienne:mise}, nous
  avons deux niveaux de risque: «bon» et «mauvais». La variable
  aléatoire $\Theta$ prend donc deux valeurs: $\theta = \frac{1}{15}$
  et $\theta = \frac{1}{10}$. Sa fonction de répartition et sa
  fonction de masse de probabilité sont, dans l'ordre:
  \begin{align*}
    U(\theta)
    &=
    \begin{cases}
      0,           & \theta < \frac{1}{15} \\
      \frac{3}{4}, &\frac{1}{15} \leq \theta < \frac{1}{10} \\
      1,           &\theta \geq \frac{1}{10}
    \end{cases} \\
    \intertext{et}
    \Pr[\Theta = \theta]
    &=
    \begin{cases}
      \frac{3}{4}, & \theta = \frac{1}{15} \\
      \frac{1}{4}, & \theta = \frac{1}{10}.
    \end{cases}
  \end{align*}
  \qed
\end{exemple}

En théorie des probabilités, notre modèle d'hétérogénéité est un
modèle dit «urne d'urne» en ce sens que les montants de sinistres
proviennent d'un processus en deux étapes; voir la
\autoref{fig:bayesienne:urne} pour une illustration.

\begin{figure}
  \centering
  \setlength{\unitlength}{2mm}
  \begin{picture}(28,16)(-5,1.5)
    \put(0,10){\LARGE\faShoppingBasket}
    \qbezier(2,12)(5,20)(9,8)
    \put(9.17,7.6){\vector(1,-2.5){0}}
    \put(8,5){\LARGE\faShoppingBasket}
    \qbezier(10,7)(13,15)(17,4)
    \put(17.17,3.6){\vector(1,-2.5){0}}
    \put(7.5,14){$\theta_i$}
    \put(16,1.5){$S_{it}$}
    \put(-1,10.5){\makebox(0,0)[r]{$U(\theta)$}}
    \put(7,5.5){\makebox(0,0)[r]{$F(x|\theta)$}}
  \end{picture}
  \caption{Modèle urne d'urne. Pour obtenir un montant de sinistre
    $S_{it}$, il faut d'abord choisir un niveau de risque $\theta_i$
    de la distribution $U(\theta)$ (première urne), puis générer un
    montant de sinistre de la distribution $F(x|\theta_i)$ (deuxième
    urne).}
  \label{fig:bayesienne:urne}
\end{figure}

\tipbox{Un modèle où la distribution d'une variable aléatoire $S$
  dépend d'un paramètre $\theta$ et que celui-ci est considéré comme
  une réalisation d'une variable aléatoire $\Theta$ est aussi connu
  sous le nom de \emph{mélange continu}. La fonction $f(x|\theta)$ est
  la fonction de vraisemblance et $\theta$ est le paramètre de
  mélange.}


\section{Prévision}
\label{sec:bayesienne:prevision}

Tel que mentionné en introduction du chapitre, notre but en
crédibilité de précision consiste à calculer la «meilleure» prime
(pure) pour chacun des contrats de notre regroupement. D'un point de
vue actuariel, cette prime est le montant espéré des sinistres d'un
contrat quelconque, éventuellement en tenant compte des observations
des années précédentes. D'un point de vue statistique, la prime peut
aussi s'interpréter comme la \emph{prévision} du montant total des
sinistres de la prochaine année.

Dans la suite, nous considérerons que la meilleure prime (ou
prévision) est celle qui minimise l'erreur quadratique moyenne.

Nous définissons trois primes: la prime de risque, la prime collective
et la prime bayésienne. Elles diffèrent par la quantité d'information
dont nous disposons sur les contrats.

\subsection{Prime de risque}
\label{sec:bayesienne:prevision:risque}

Si le niveau de risque du contrat $i$ est connu, alors la meilleure
prévision de ses sinistres futurs est l'espérance
\begin{equation}
  \label{eq:bayesienne:prime_risque}
  \mu(\theta_i)
  = \esp{S_{it}|\Theta_i = \theta_i}
  = \int_0^\infty x f(x|\theta_i)\, dx.
\end{equation}
Cette fonction est appelée la \emph{prime de risque}.

La prime de risque serait la prime idéale à charger. Cependant, le
niveau de risque $\theta_i$ et, donc, la prime de risque sont
inconnus. Dès lors, prévoir le niveau des sinistres de la prochaine
période, $S_{i, n + 1}$, et prévoir la prime de risque $\mu(\theta_i)$
deviennent des problèmes équivalents.

\subsection{Prime collective}
\label{sec:bayesienne:prevision:collective}

Nous cherchons à estimer --- ou prévoir --- la prime de risque. Comme
première approximation, nous pouvons utiliser la moyenne pondérée de
toutes les primes de risque:
\begin{equation}
  \label{eq:bayesienne:prime_collective}
  m
  = \esp{\mu(\Theta_i)}
  = \int_{-\infty}^\infty \mu(\theta) u(\theta)\, d\theta.
\end{equation}
Cette approximation sera la même pour tous les contrats; c'est la
\emph{prime collective}.

Vous remarquerez que
\begin{equation*}
  m = \esp{\mu(\Theta_i)} = \esp{\esp{S_{it}|\Theta_i}} = \esp{S_{it}}.
\end{equation*}
La prime collective est donc aussi égale au montant espéré des
sinistres dans le portefeuille.

\subsection{Prime bayésienne}
\label{sec:bayesienne:prevision:bayesienne}

Comme nous l'avons vu au \autoref{chap:introduction-historique}, la
prime collective ci-dessus est globalement adéquate, mais elle n'est
pas nécessairement équitable ou \emph{optimale}. Nous ne pouvons
établir ce fait qu'à partir du moment où des données de sinistres
deviennent disponibles. En termes statistiques, cela signifie qu'il
existe une meilleure approximation des primes de risque dans ces
circonstances.

La meilleure approximation (ou estimation, ou prévision) de la prime
de risque $\mu(\theta_i)$ est la fonction $g^*(x_{i1}, \dots, x_{in})$
des observations $x_{i1}, \dots, x_{in}$ du contrat $i$ qui minimise
l'erreur quadratique moyenne
\begin{equation*}
  \esp{(\mu(\Theta_i) - g(x_{i1}, \dots, x_{in}))^2},
\end{equation*}
où $g()$ est une fonction quelconque.

Vous trouverez dans tout livre de statistique mathématique la
démonstration à l'effet que
\begin{equation*}
  g^*(x_{i1}, \dots, x_{in}) = \esp{\mu(\Theta_i)|x_{i1}, \dots, x_{in}}.
\end{equation*}
Nous nommerons cette espérance la \emph{prime bayésienne}:
\begin{equation}
  \label{eq:bayesienne:prime_bayesienne}
  B_{i, n + 1}
  = \esp{\mu(\Theta_i)|S_{i1} = x_{i1}, \dots, S_{in} = x_{in}}
  = \int_{-\infty}^\infty \mu(\theta)
  u(\theta|x_{i1}, \dots, x_{in})\, d\theta.
\end{equation}

\importantbox{La prime bayésienne est la meilleure prévision de
  $S_{i, n + 1}$ que l'on puisse calculer.}

Ci-dessus, $u(\theta|x_{i1}, \dots, x_{in})$ est la distribution à
postériori des niveaux de risque. En d'autres termes,
$U(\theta|x_{i1}, \dots, x_{in})$ est la fonction de structure révisée
après l'observation de l'expérience
$S_{i1} = x_{i1}, \dots, S_{in} = x_{in}$. Or, par la règle de Bayes
et étant donné l'indépendance conditionnelle des observations,
\begin{align*}
  u(\theta_i|x_{i1}, \dots, x_{in})
  &= \frac{f(x_{i1}, \dots, x_{in}|\theta_i) u(\theta_i)}{%
    \int_{-\infty}^\infty
    f(x_{i1}, \dots, x_{in}|\theta) u(\theta)\, d\theta} \\
  &= \frac{\prod_{t = 1}^n f(x_{it}|\theta_i) u(\theta_i)}{%
    \int_{-\infty}^\infty \prod_{t = 1}^n
    f(x_{it}|\theta) u(\theta)\, d\theta} \\
  &\propto\;
  u(\theta_i) \prod_{t = 1}^n f(x_{it}|\theta_i).
\end{align*}

Comme la prime collective, la prime bayésienne est une moyenne
pondérée des primes de risque, sauf que cette dernière utilise la
distribution à postériori de $\Theta$ plutôt que la distribution à
priori. En effet, comparez les deux équations ci-dessous:
\begin{align*}
  m &= \int_{-\infty}^\infty \mu(\theta) u(\theta)\, d\theta \\
  B_{i, n + 1} &= \int_{-\infty}^\infty \mu(\theta)
                 u(\theta|x_{i1}, \dots, x_{in})\, d\theta.
\end{align*}

À l'inverse, nous pouvons interpréter la prime collective comme la
prime bayésienne de première année, lorsque aucune expérience n'est
disponible.

\tipbox{Puisque les montants de sinistres apparaissent seulement sous
  forme de produit dans la fonction de vraisemblance
  \begin{equation*}
    f(x_{i1}, \dots, x_{in}|\theta_i) = \prod_{t = 1}^n f(x_{it}|\theta_i),
  \end{equation*}
  leur ordre ne joue aucun rôle dans les calculs.}

\importantbox{Dans la suite, nous n'allons considérer qu'un seul
  contrat et, par conséquent, nous laisserons tomber l'indice $i$ dans
  la notation. En effet, dans l'approche entièrement paramétrique
  adoptée dans ce chapitre, le portefeuille de contrats ne joue aucun
  rôle concret.}

\begin{exemple}
  \label{ex:bayesienne:poisson-discrete}
  Soit $S_t|\Theta = \theta \sim \text{Poisson}(\theta)$ et
  \begin{equation*}
    \Pr[\Theta = \theta] =
    \begin{cases}
      0,3, & \theta = \frac{1}{2} \\
      0,5, & \theta = 1 \\
      0,2, & \theta = 2.
    \end{cases}
  \end{equation*}
  Nous allons calculer les trois primes définies précédemment.

  \begin{enumerate}[a)]
  \item Puisque $\esp{S_t|\Theta = \theta} = \theta$, les primes de
    risque pour chacun des trois niveaux de risque sont:
    \begin{align*}
      \mu({\tfrac{1}{2}}) &= \tfrac{1}{2} \\
      \mu(1) &= 1 \\
      \mu(2) &= 2.
    \end{align*}
  \item La prime collective est la moyenne pondérée des primes de
    risque:
    \begin{align*}
      m
      &= \esp{\mu(\Theta)} \\
      &= \esp{\Theta} \\
      &= \frac{1}{2} (0,3) + 1 (0,5) + 2 (0,2) \\
      &= 1,05.
    \end{align*}
    Nous pourrions également calculer la prime collective avec la
    formule $m = \esp{S_t}$, mais la distribution marginale de $S_t$
    est inconnue.
  \item Supposons que $S_1 = 2$ et $S_2 = 1$. Pour calculer la prime
    bayésienne pour la troisième année, nous devons d'abord calculer
    la distribution à postériori de $\Theta$:
    \begin{equation*}
      \Pr[\Theta = \theta|S_1 = 2, S_2 = 1] =
      \frac{\Pr[S_1 = 2, S_2 = 1|\Theta = \theta] \Pr[\Theta = \theta]}
      {\sum_{\theta} \Pr[S_1 = 2, S_2 = 1|\Theta = \theta] \Pr[\Theta = \theta]}
    \end{equation*}
    Or,
    \begin{align*}
      \Pr[S_1 = 2, S_2 = 1|\Theta = \theta]
      &= \Pr[S_1 = 2|\Theta = \theta] \Pr[S_2 = 1|\Theta = \theta] \\
      &= \frac{\theta^2 e^{-\theta}}{2!} \frac{\theta e^{-\theta}}{1!} \\
      &= \frac{\theta^3 e^{-2\theta}}{2}
    \end{align*}
    et donc
    \begin{equation*}
      \Pr[\Theta = \theta|S_1 = 2, S_2 = 1] =
      \begin{cases}
        0,1245, & \theta = \frac{1}{2} \\
        0,6109, & \theta = 1 \\
        0,2646, & \theta = 2.
      \end{cases}
    \end{equation*}
    La prime bayésienne est donc:
    \begin{align*}
      B_3
      &= \esp{\mu(\Theta)|S_1 = 2, S_2 = 1} \\
      &= \esp{\Theta|S_1 = 2, S_2 = 1} \\
      &= \frac{1}{2} (0,1245) + 1 (0,6109) + 2 (0,2646) \\
      &= 1,20.
    \end{align*}
  \end{enumerate}

  Remarquez que, dans le cas présent, la prime bayésienne se trouve
  entre la prime collective et la moyenne des observations au cours
  des deux premières années: $1,05 = m < B_3 < \bar{S} = 1,5$. Ce
  n'est pas toujours le cas avec la prime bayésienne. %
  \qed
\end{exemple}

\begin{exemple}[Bernoulli/bêta]
  \label{ex:bayesienne:bernoulli-beta}
  Examinons de nouveau le portefeuille simplifié de
  l'\autoref{ex:introduction-historique:simplifie}, cette fois dans le
  cadre théorique que nous venons de mettre en place. Dans cet
  exemple, un contrat ne peut avoir au maximum qu'un seul sinistre de
  montant $1$ par année. (En d'autres termes, l'expérience consiste en
  une suite de $1$ et de $0$ selon qu'il y a eu un sinistre dans une
  année ou non.) La probabilité d'avoir un sinistre est toutefois
  inconnue et potentiellement différente pour chaque contrat.

  Nous avons $S_t|\Theta = \theta \sim \text{Bernoulli}(\theta)$ et
  nous supposons que le paramètre $\theta$ est une réalisation d'une
  variable aléatoire avec distribution bêta de paramètres $a$ et
  $b$. Ainsi,
  \begin{equation*}
    f(x|\theta) = \theta^x (1 - \theta)^{1 - x},
    \quad x = 0, 1
  \end{equation*}
  et
  \begin{equation*}
    u(\theta)
    = \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)}\,
    \theta^{a - 1} (1 - \theta)^{b - 1},
    \quad 0 < \theta < 1.
  \end{equation*}

  \begin{enumerate}[a)]
  \item La prime de risque pour un contrat est
    $\esp{S_t|\Theta = \theta} = \theta$.
  \item La prime collective est, comme toujours, la moyenne des primes
    de risque:
    \begin{align*}
      m
      &= \esp{\mu(\Theta)} \\
      &= \esp{\Theta} \\
      &= \frac{a}{a + b}.
    \end{align*}
  \item Nous souhaitons maintenant calculer la prime bayésienne après
    $n$ années d'observations $x_1, \dots, x_n$. Pour ce faire, nous
    déterminons d'abord la distribution à postériori de $\Theta$. À
    une constante de proportionnalité, celle-ci est:
    \begin{align*}
      u(\theta|x_1, \dots, x_n)
      &\propto
        u(\theta_i) \prod_{t = 1}^n f(x_t|\theta_i) \\
      &\propto \theta^{a - 1} (1 - \theta)^{b - 1}
        \prod_{t = 1}^n \theta^{x_t} (1 - \theta)^{1 - x_t} \\
      &= \theta^{a + \sum_{t = 1}^n x_t - 1} (1 - \theta)^{b +
        n - \sum_{t = 1}^n x_t - 1}.
    \end{align*}
    Nous reconnaissons là une distribution bêta, mais avec de nouveaux
    paramètres $\tilde{a} = a + \sum_{t = 1}^n x_t$ et
    $\tilde{b} = b + n - \sum_{t = 1}^n x_t$. Par conséquent,
    la prime bayésienne pour l'année $n + 1$ est
    \begin{align*}
      B_{n + 1}
      &= \esp{\mu(\Theta)|S_1, \dots, S_n} \\
      &= \esp{\Theta|S_1, \dots, S_n} \\
      &= \frac{\tilde{a}}{\tilde{a} + \tilde{b}} \\
      &= \frac{a + \sum_{t = 1}^n S_t}{a + b + n}.
    \end{align*}
  \end{enumerate}
  \qed
\end{exemple}

\begin{exemple}
  \label{ex:bayesienne:bernoulli-uniforme}
  Reprenons le modèle de l'\autoref{ex:bayesienne:bernoulli-beta},
  soit une Bernoulli de paramètre $\theta$ pour la distribution de la
  variable aléatoire $S_t|\Theta = \theta$, mais en changeant la
  distribution de la variable aléatoire $\Theta$ pour une uniforme sur
  l'intervalle $(a, b)$.

  Soit $n \bar{S} = \sum_{t = 1}^n S_t$ le montant total des sinistres
  d'un contrat après $n$ années. \citet{Norberg:credibility:1979}
  démontre que la prime bayésienne avec ce modèle est
  \begin{equation*}
    B_{n + 1} =
    \dfrac{\sum_{j = 1}^{n - n \bar{S}} (-1)^j
      \frac{b^{n \bar{S} + j + 2} - a^{n \bar{S} + j + 2}}
      {(n - n \bar{S} - j)!\, j!\, (n \bar{S} + j + 2)}}
    {\sum_{j = 1}^{n - n \bar{S}} (-1)^j
      \frac{b^{n \bar{S} + j + 1} - a^{n \bar{S} + j + 1}}
      {(n - n \bar{S} - j)!\, j!\, (n \bar{S} + j + 1)}}.
  \end{equation*}

  Cette prime bayésienne n'est pas linéaire et, de plus, elle ne se
  trouve pas nécessairement entre l'expérience individuelle $\bar{S}$
  et la prime collective $m$. %
  \qed
\end{exemple}


\section{Approche par la distribution prédictive}
\label{sec:bayesienne:predictive}

Cette section introduit une manière alternative de calculer la prime
bayésienne. Nous avons déjà effleuré l'idée à la
\autoref{sec:bayesienne:prevision:collective} lorsque nous avons
présenté la prime collective à la fois comme la moyenne des primes de
risque ou comme le montant espéré des sinistres:
\begin{equation*}
  m = \esp{\mu(\Theta)} = \esp{S_{it}}.
\end{equation*}
De manière similaire, nous pouvons démontrer que
\begin{equation}
  \label{eq:bayesienne:predictive}
  B_{i, n + 1} = \esp{\mu(\Theta)|S_{i1} = x_1, \dots, S_{in} = x_n}
  = \esp{S_{i, n + 1}|S_{i1} = x_1, \dots, S_{in} = x_n}.
\end{equation}

La distribution de la variable aléatoire
$S_{n + 1}|S_1 = x_1, \dots, S_n = x_1$ avec fonction de densité de
probabilité $f(x|x_1, \dots, x_n)$ est appelée la \emph{distribution
  prédictive} de la variable aléatoire $S_{n + 1}$.

\begin{thm}
  \label{thm:bayesienne:predictive}
  La fonction de densité de probabilité de la distribution prédictive
  de $S_{n + 1}$ est
  \begin{equation*}
    f(x|x_1, \dots, x_n) = \int_{-\infty}^\infty f(x|\theta)
    u(\theta|x_1, \dots, x_n)\, d\theta.
  \end{equation*}
\end{thm}
\begin{proof}
  Par la règle de Bayes et par l'indépendance conditionnelle des
  montants de sinistres, nous obtenons:
  \begin{align*}
    f(x|x_1, \dots, x_n)
    &= \frac{f(x, x_1, \dots, x_n)}{f(x_1, \dots, x_n)} \\
    &= \frac{%
      \int_{-\infty}^\infty f(x, x_1, \dots, x_n|\theta) u(\theta)\,
      d\theta}{%
      \int_{-\infty}^\infty f(x_1, \dots, x_n|\theta)
      u(\theta)\, d\theta} \\
    &= \int_{-\infty}^\infty f(x|\theta)\, \frac{f(x_1, \dots,
      x_n|\theta) u(\theta)}{%
      \int_{-\infty}^\infty f(x_1, \dots, x_n|\theta)
      u(\theta)\, d\theta}\, d\theta \\
    &= \int_{-\infty}^\infty f(x|\theta) u(\theta|x_1, \dots, x_n)\,
    d\theta.
  \end{align*}
\end{proof}

Puisque la fonction de densité de probabilité de la distribution
marginale de la variable aléatoire $S_t$ est
\begin{equation*}
  f(x) = \int_{-\infty}^\infty f(x|\theta) u(\theta)\, d\theta,
\end{equation*}
alors la seule différence entre l'expression de $f(x)$ et celle de
$f(x|x_1, \dots, x_n)$ réside dans l'utilisation de la distribution à
priori de $\Theta$ pour la première et de la distribution à postériori
pour la seconde.

\tipbox{\label{tip:baysienne:predictive} La conséquence de la remarque
  précédente, c'est que si la distribution à posteriori de $\Theta$
  est du même type que la distribution à priori, alors la distribution
  prédictive est du même type que la distribution marginale.}

Nous avons posé la relation \eqref{eq:bayesienne:predictive} de
manière quelque peu intuitive. Démontrons maintenant qu'elle est
vraie. Nous savons déjà que
\begin{equation*}
  \esp{\mu(\Theta)|S_{i1} = x_1, \dots, S_{in} = x_n}
  = \int_{-\infty}^\infty
  \mu(\theta) u(\theta|x_1, \dots, x_n)\, d\theta
\end{equation*}
et
\begin{equation*}
  \mu(\theta) = \int_0^\infty x f(x|\theta)\, dx.
\end{equation*}
Par conséquent,
\begin{align*}
  \esp{\mu(\Theta)|S_{i1} = x_1, \dots, S_{in} = x_n}
  &= \int_{-\infty}^\infty \int_0^\infty
  x f(x|\theta) u(\theta|x_1, \dots, x_n)\, dx\, d\theta \\
  &= \int_0^\infty
  x
  \left(
    \int_{-\infty}^\infty
    f(x|\theta) u(\theta|x_1, \dots, x_n)\, d\theta
  \right)
  dx
\end{align*}
d'où, par le \autoref{thm:bayesienne:predictive},
\begin{align*}
  \esp{\mu(\Theta)|S_{i1} = x_1, \dots, S_{in} = x_n}
  &= \int_0^\infty x f(x|x_1, \dots, x_n)\, dx \\
  &= \esp{S_{i, n + 1}|S_{i1} = x_1, \dots, S_{in} = x_n}.
\end{align*}

Avec cette approche, la prime collective et la prime bayésienne
s'interprètent toutes deux comme le montant moyen des sinistres dans
le portefeuille, mais avec des pondérations différentes.

\begin{exemple}[Poisson/gamma]
  \label{ex:bayesienne:poisson-gamma}
  Le présent exemple joue un rôle important dans la suite. Soit
  \begin{align*}
    S_t|\Theta = \theta &\sim \text{Poisson}(\theta) \\
                 \Theta &\sim \text{Gamma}(\alpha, \lambda),
  \end{align*}
  c'est-à-dire
  \begin{align*}
    f(x|\theta)
    &= \frac{\theta^x e^{-\theta}}{x!}, \quad x = 0, 1, \dots \\
    u(\theta)
    &= \frac{\lambda^\alpha}{\Gamma(\alpha)}\,
    \theta^{\alpha - 1} e^{-\lambda \theta}, \quad \theta > 0.
  \end{align*}
  Comme dans les exemples \ref{ex:bayesienne:poisson-discrete} et
  \ref{ex:bayesienne:bernoulli-beta}, nous allons calculer les primes
  de risque, collective et bayésienne. Nous disposons toutefois
  maintenant de l'approche par la distribution prédictive pour
  effectuer nos calculs.

  \begin{enumerate}[a)]
  \item La prime de risque est
    \begin{equation*}
      \mu(\theta) = \esp{S_t|\Theta = \theta} = \theta.
    \end{equation*}
    Dans la suite, nous aurons également besoin de la variance
    conditionnelle
    \begin{equation*}
      \sigma^2(\theta) = \var{S_t|\Theta = \theta} = \theta.
    \end{equation*}

  \item La prime collective est
    \begin{equation*}
      m = \esp{\mu(\Theta)} = \esp{\Theta} = \frac{\alpha}{\lambda}.
    \end{equation*}

  \item Calculons maintenant la prime bayésienne à partir de la
    distribution à postériori de $\Theta$. Tout d'abord, nous avons
    que
    \begin{align*}
      u(\theta|x_1, \dots, x_n)
      &\propto u(\theta) \prod_{t = 1}^n f(x_t|\theta) \\
      &\propto \theta^{\alpha - 1} e^{-\lambda \theta}
      \prod_{t = 1}^n \theta^{x_t} e^{-\theta} \\
      &= \theta^{\alpha + \sum_{t = 1}^n x_t - 1}
      e^{-(\lambda + n) \theta},
    \end{align*}
    d'où la distribution à postériori de $\Theta$ est aussi une
    distribution gamma, mais avec de nouveaux paramètres:
    \begin{equation*}
      \Theta|S_1, \dots, S_n \sim \text{Gamma}
      \left(
        \tilde{\alpha} = \alpha + \sum_{t = 1}^n S_t,
        \tilde{\lambda} = \lambda + n
      \right).
    \end{equation*}
    Par conséquent, la prime bayésienne est
    \begin{align*}
      B_{n + 1}
      &= \esp{\mu(\Theta)|S_1, \dots, S_n} \\
      &= \esp{\Theta|S_1, \dots, S_n} \\
      &= \frac{\tilde{\alpha}}{\tilde{\lambda}} \\
      &= \frac{\alpha + \sum_{t = 1}^n S_t}{\lambda + n}
    \end{align*}

  \item Avant d'examiner la distribution prédictive, calculons la
    distribution marginale de $S_t$. Nous avons
    \begin{align*}
      f(x)
      &= \int_0^\infty f(x|\theta) u(\theta)\, d\theta \\
      &= \frac{\lambda^\alpha}{\Gamma(\alpha) x!}
      \int_0^\infty \theta^x e^{-\theta}
      \theta^{\alpha - 1} e^{-\lambda \theta}\, d\theta \\
      &= \frac{\lambda^\alpha}{\Gamma(\alpha) \Gamma(x + 1)}
      \int_0^\infty \theta^{\alpha + x - 1}
      e^{-(\lambda + 1) \theta}\, d\theta \\
      &= \frac{\lambda^\alpha}{\Gamma(\alpha) \Gamma(x + 1)}
      \frac{\Gamma(\alpha + x)}{(\lambda + 1)^{\alpha + x}} \\
      &= \frac{\Gamma(\alpha + x)}{\Gamma(\alpha) \Gamma(x + 1)}
      \left(
        \frac{\lambda}{\lambda + 1}
      \right)^\alpha
      \left(
        \frac{1}{\lambda + 1}
      \right)^x \\
      &= \binom{\alpha + x - 1}{\alpha - 1}
        \left(
          \frac{\lambda}{\lambda + 1}
        \right)^\alpha
        \left(
          \frac{1}{\lambda + 1}
        \right)^x
        \quad x = 0, 1, \dots.
    \end{align*}
    Nous identifions cette fonction de masse de probabilité comme
    celle d'une binomiale négative de paramètres $r = \alpha$ et
    $p = \lambda/(\lambda + 1)$ (voir la
    \autoref{sec:distributions:binomialeneg}).

    Nous pouvons obtenir le même résultat par les fonctions
    génératrices des moments. En effet,
    \begin{align*}
      M_S(t)
      &= \esp{e^{t S}} \\
      &= \esp{\esp{e^{t S}|\Theta}} \\
      &= \esp{M_{S|\Theta}(t)} \\
      &= \esp{e^{\Theta (e^t - 1)}} \\
      &= M_\Theta(e^t - 1) \\
      &=
      \left(
        \frac{\lambda}{\lambda + 1 - e^t}
      \right)^\alpha \\
      &=
      \left(
        \frac{\lambda/(\lambda + 1)}{1 - e^t/(\lambda + 1)}
      \right)^\alpha,
    \end{align*}
    ce qui est la fonction génératrice des moments d'une binomiale
    négative de paramètres $\alpha$ et $\lambda/(\lambda + 1)$.

    Ce lien entre les lois de Poisson, gamma et binomiale négative
    constitue un résultat classique en théorie des probabilités.
    Répétons-le pour le mettre en exergue: si
    \begin{align*}
      S|\Theta = \theta &\sim \text{Poisson}(\theta) \\
      \Theta &\sim \text{Gamma}(\alpha, \lambda)
    \end{align*}
    alors
    \begin{equation*}
      S \sim \text{Binomiale négative}
      \left(
        \alpha, \frac{\lambda}{\lambda + 1}
      \right).
    \end{equation*}

    Des résultats ci-dessus découle que la prime collective est
    \begin{equation*}
      m = \esp{S_t} = \frac{\alpha (1 - \lambda/(\lambda + 1))}{\lambda/(\lambda
      + 1)} = \frac{\alpha}{\lambda},
    \end{equation*}
    soit le même résultat que précédemment.

  \item Les distributions à priori et à postériori de $\Theta$
    sont toutes deux des distributions gamma, la seconde avec des
    paramètres mis à jour suite à l'ajout de données:
    \begin{equation*}
      \Theta \sim \text{Gamma}(\alpha, \lambda)
    \end{equation*}
    et
    \begin{equation*}
      \Theta|S_1 = x_1, \dots, S_n = x_n
      \sim \text{Gamma}(\tilde{\alpha}, \tilde{\lambda}).
    \end{equation*}
    C'est le contexte de la remarque de la
    \autopageref{tip:baysienne:predictive}. Nous pouvons directement
    établir que la distribution prédictive de $S_{n + 1}$ est:
    \begin{equation*}
      S_{n + 1}|S_1 = x_1, \dots, S_n = x_n
      \sim \text{Binomiale négative}(\tilde{r}, \tilde{p})
    \end{equation*}
    avec
    \begin{align*}
      \tilde{r}
      &= \tilde{\alpha} = \alpha + \sum_{t = 1}^n x_t \\
      \tilde{p}
      &= \frac{\tilde{\lambda}}{\tilde{\lambda} + 1} =
      \frac{\lambda + n}{\lambda + n + 1}.
    \end{align*}
    Par conséquent, la prime bayésienne, calculée cette fois à partir
    de la distribution prédictive, est:
    \begin{align*}
      B_{n + 1}
      &= \esp{S_{n + 1}|S_1, \dots, S_n} \\
      &= \frac{\tilde{\alpha}}{\tilde{\lambda}} \\
      &= \frac{\alpha + \sum_{t = 1}^n S_t}{\lambda + n}.
    \end{align*}

  \item Afin de fournir un autre exemple de calculs équivalents, mais
    qui empruntent un chemin différent, calculons $\var{S}$. En
    premier lieu, nous pouvons procéder en conditionnant sur $\Theta$:
    \begin{align*}
      \var{S}
      &= \esp{\var{S|\Theta}} + \var{\esp{S|\Theta}} \\
      &= \esp{\sigma^2(\Theta)} + \var{\mu(\Theta)} \\
      &= \esp{\Theta} + \var{\Theta} \\
      &= \frac{\alpha}{\lambda} + \frac{\alpha}{\lambda^2} \\
      &= \frac{\alpha (\lambda + 1)}{\lambda^2}.
    \end{align*}
    En second lieu, nous pouvons calculer la variance directement
    depuis la marginale:
    \begin{align*}
      \var{S}
      &= \frac{r (1 - p)}{p^2} \\
      &= \frac{\alpha (\lambda + 1)}{\lambda^2}.
    \end{align*}

  \item Le \autoref{tab:bayesienne:poisson-gamma} contient les
    résultats de la prime bayésienne des dix premières années si la
    distribution à priori de la variable aléatoire $\Theta$ est une
    gamma de paramètres $\alpha = 3$ et $\lambda = 3$, et que les
    montants de sinistres au cours de ces années sont les suivants:
    $5, 3, 0, 1, 1, 2, 0, 2, 0, 2$.

    La \autoref{fig:bayesienne:poisson-gamma} illustre que la
    distribution à postériori de $\Theta$ est de plus en plus
    concentrée autour de sa moyenne au fur et à mesure que
    l'expérience s'accumule. En d'autres termes, l'incertitude sur le
    niveau de risque du contrat sous étude diminue. Du même coup, la
    précision de la prime bayésienne s'améliore. En effet, la vraie
    valeur de $\theta$ est utilisée pour générer les sinistres
    ci-dessus est $1,48$. %
    \qed
  \end{enumerate}

  \begin{table}
    \centering
    \caption{Primes bayésiennes de
      l'\autoref{ex:bayesienne:poisson-gamma}}
    \label{tab:bayesienne:poisson-gamma}
    \begin{tabular}{*{5}{>{$}r<{$}}>{$}l<{$}}
      \toprule
      n &
      x_n &
      \sum x_t &
      \alpha + \sum x_t &
      \lambda + n &
      B_{n+1} \\
      \midrule
      0 & $--$ & $--$ &  3 &  3 & 1,0     \\
      1 &  5 &  5 &  8 &  4 & 2,0     \\
      2 &  3 &  8 & 11 &  5 & 2,2   \\
      3 &  0 &  8 & 11 &  6 & 1,83  \\
      4 &  1 &  9 & 12 &  7 & 1,71  \\
      5 &  1 & 10 & 13 &  8 & 1,625 \\
      6 &  2 & 12 & 15 &  9 & 1,667 \\
      7 &  0 & 12 & 15 & 10 & 1,5   \\
      8 &  2 & 14 & 17 & 11 & 1,54  \\
      9 &  0 & 14 & 17 & 12 & 1,42  \\
      10 &  2 & 16 & 19 & 13 & 1,46  \\
      \bottomrule
    \end{tabular}
  \end{table}

  \begin{figure}[t]
    \setkeys{Gin}{width=\textwidth}
    \begin{minipage}[t]{0.45\textwidth}
      \centering
<<echo=FALSE, fig=TRUE, width=5, height=5>>=
par(mar = c(3, 2, 3, 2))
curve(dgamma(x, 3, 3), xlim = c(0, 4), main = "À priori")
@
      \subcaption{Gamma avec $\alpha = 3$ et $\lambda =
        3$. L'espérance est égale à $1$.}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
      \centering
<<echo=FALSE, fig=TRUE, width=5, height=5>>=
par(mar = c(3, 2, 3, 2))
curve(dgamma(x, 8, 4), xlim = c(0, 4), main = "Après 1 année")
@
      \subcaption{Gamma avec $\alpha = 8$ et $\lambda =
        4$. L'espérance est égale à $2$.}
    \end{minipage}
    \begin{minipage}[t]{0.45\textwidth}
      \centering
<<echo=FALSE, fig=TRUE, width=5, height=5>>=
par(mar = c(3, 2, 3, 2))
curve(dgamma(x, 13, 8), xlim = c(0, 4), main = "Après 5 années")
@
      \subcaption{Gamma avec $\alpha = 13$ et $\lambda =
        8$. L'espérance est égale à $1,625$.}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
      \centering
<<echo=FALSE, fig=TRUE, width=5, height=5>>=
par(mar = c(3, 2, 3, 2))
curve(dgamma(x, 19, 13), xlim = c(0, 4), main = "Après 10 années")
@
      \subcaption{Gamma avec $\alpha = 19$ et $\lambda =
        13$. L'espérance est égale à $1,46$.}
    \end{minipage}
    \caption{Distributions à priori et à postériori de la variable
      aléatoire $\Theta$ dans l'\autoref{ex:bayesienne:poisson-gamma}}
    \label{fig:bayesienne:poisson-gamma}
  \end{figure}
\end{exemple}


\section{Crédibilité bayésienne linéaire}
\label{sec:bayesienne:lineaire}

Examinons de plus près la prime bayésienne obtenue à
l'\autoref{ex:bayesienne:bernoulli-beta} avec une fonction de
vraisemblance Bernoulli et une distribution du paramètre de mélange
bêta. Nous avons obtenu
\begin{equation*}
  B_{n + 1} = \frac{\alpha + \sum_{t = 1}^n S_t}{\alpha + \beta + n}.
\end{equation*}
Or, cette expression se réécrire ainsi:
\begin{align*}
  B_{n + 1}
  %% &= \frac{n}{n + \alpha + \beta}\, \frac{\sum_{t = 1}^n S_t}{n} +
  %% \frac{\alpha}{n + \alpha + \beta} \\
  &= \frac{n}{n + \alpha + \beta}\, \bar{S} +
  \frac{\alpha + \beta}{n + \alpha + \beta} \frac{\alpha}{\alpha + \beta} \\
  &= z \bar{S} + (1 - z) m,
\end{align*}
où $\bar{S} = n^{-1} \sum_{t = 1}^n S_t$, $m$ est la prime collective
et
\begin{equation*}
  z = \frac{n}{n + K}, \quad K = \alpha + \beta.
\end{equation*}

Nous pouvons répéter le même exercice avec les résultats de
l'\autoref{ex:bayesienne:poisson-gamma} pour la combinaison d'une loi
de Poisson et d'une loi gamma. Dans ce cas,
\begin{equation*}
  B_{n + 1} = \frac{\alpha + \sum_{t = 1}^n S_t}{\lambda + n},
\end{equation*}
soit
\begin{align*}
  B_{n + 1}
  %% &= \frac{n}{n + \lambda}\, \frac{\sum_{t = 1}^n S_t}{n} +
  %%   \frac{\alpha}{n + \lambda} \\
  &= \frac{n}{n + \lambda}\, \bar{S} +
    \frac{\lambda}{n + \lambda} \frac{\alpha}{\lambda} \\
  &= z \bar{S} + (1 - z) m,
\end{align*}
avec
\begin{equation*}
  z = \frac{n}{n + K}, \quad K = \lambda.
\end{equation*}

Ces résultats sont très séduisants sous plusieurs aspects.
Premièrement, la prime bayésienne est linéaire, ce qui est à la fois
simple à calculer et facile à interpréter --- rappelez-vous à cet
effet le cas tout à fait contraire de
l'\autoref{ex:bayesienne:bernoulli-uniforme}. Ensuite, puisque
$0 < z < 1$ pour $n > 0$, la prime bayésienne se trouve être une
combinaison convexe de $\bar{S}$ et $m$. Elle se situe donc toujours
entre l'expérience individuelle d'un contrat ($\bar{S}$) et la prime
collective ($m$). D'un point de vue actuariel, il s'agit d'une
propriété importante. Imaginez seulement, si ce n'était pas le cas,
devoir expliquer à un client dont l'expérience est pire que la moyenne
que vous devez non seulement lui charger plus que la prime collective,
mais aussi plus que sa propre expérience!

Les primes de forme
\begin{equation*}
  \pi_{n + 1} = z \bar{S} + (1 - z) m,
\end{equation*}
où $0 \leq z \leq 1$, jouent un rôle fondamental en théorie de la
crédibilité. Nous les appelons \emph{primes de crédibilité} et $z$ est
le \emph{facteur de crédibilité}.

\citet{Whitney:1918} et \citet{Bailey:1950} furent les premiers à
démontrer que la prime bayésienne est une prime de crédibilité pour
certaines combinaisons de distributions. Nous en avons déjà identifié
deux. Il y a en fait cinq combinaisons de distributions qui résultent
en une prime bayésienne linéaire (en plus de leurs convolutions):
\begin{enumerate}
\item $S|\Theta = \theta \sim \text{Poisson}(\theta)$ et $\Theta \sim
  \text{Gamma}(\alpha, \lambda)$;
\item $S|\Theta = \theta \sim \text{Exponentielle}(\theta)$ et $\Theta \sim
  \text{Gamma}(\alpha, \lambda)$;
\item $S|\Theta = \theta \sim \text{Normale}(\theta, \sigma^2_2)$ et $\Theta \sim
  \text{Normale}(\mu, \sigma^2_1)$;
\item $S|\Theta = \theta \sim \text{Bernoulli}(\theta)$ et $\Theta \sim
  \text{Bêta}(a, b)$;
\item $S|\Theta = \theta \sim \text{Géométrique}(\theta)$ et $\Theta \sim
  \text{Bêta}(a, b)$.
\end{enumerate}

Nous effectuons les calculs de prime pour les cas exponentielle/gamma
et normale/normale dans les deux exemples suivants. Vous devez faire
de même pour le cas géométrique/bêta à
l'\autoref{exercice:bayesienne:geometrique-beta}.

\begin{exemple}[Exponentielle/gamma]
  \label{ex:bayesienne:poisson-exponentielle}
  Soit
  \begin{align*}
    S_t|\Theta = \theta &\sim \text{Exponentielle}(\theta) \\
                 \Theta &\sim \text{Gamma}(\alpha, \lambda).
  \end{align*}

  La prime de risque est
  \begin{equation*}
    \mu(\theta) = \frac{1}{\theta}
  \end{equation*}
  et la variance conditionnelle est
  \begin{equation*}
    \sigma^2(\theta) = \frac{1}{\theta^2}.
  \end{equation*}

  La prime collective est
  \begin{align*}
    m
    &= \Esp{\frac{1}{\Theta}} \\
    &= \frac{\lambda^\alpha}{\Gamma(\alpha)} \int_0^\infty
    \theta^{\alpha - 1 - 1} e^{-\lambda \theta}\, d\theta \\
    &= \frac{\lambda^\alpha}{\Gamma(\alpha)}
    \frac{\Gamma(\alpha - 1)}{\lambda^{\alpha - 1}} \\
    &= \frac{\lambda}{\alpha - 1}.
  \end{align*}

  La distribution à postériori de $\Theta$ est
  \begin{align*}
    u(\theta|x_1, \dots, x_n)
    &\propto \theta^{\alpha - 1} e^{-\lambda \theta}
    \prod_{t = 1}^n \theta e^{-\theta x_t} \\
    &= \theta^{\alpha + n - 1}
    e^{-(\lambda + \sum_{t = 1}^n x_t) \theta},
  \end{align*}
  d'où $\Theta|S_1, \dots, S_n \sim \text{Gamma}(\tilde{\alpha} =
  \alpha + n, \tilde{\lambda} = \lambda + \sum_{t = 1}^n S_t)$.

  Pour les distributions marginale et prédictive, nous avons
  \begin{align*}
    f(x)
    &= \frac{\lambda^\alpha}{\Gamma(\alpha)}  \int_0^\infty
    \theta e^{-\theta}
    \theta^{\alpha - 1} e^{-\lambda \theta}\, d\theta \\
    &= \frac{\lambda^\alpha}{\Gamma(\alpha)} \int_0^\infty
    \theta^{\alpha + 1 - 1} e^{-(\lambda + x) \theta}\, d\theta \\
    &= \frac{\lambda^\alpha}{\Gamma(\alpha)}
    \frac{\Gamma(\alpha + 1)}{(\lambda + x)^{\alpha + 1}} \\
    &= \frac{\alpha \lambda^\alpha}{(\lambda + x)^{\alpha + 1}}, \quad
    x > 0,
  \end{align*}
  d'où $S_t \sim \text{Pareto}(\alpha, \lambda)$ et
  \begin{equation*}
    \textstyle
    S_{n + 1}|S_1 = x_1, \dots, S_n = x_n \sim
    \text{Pareto}(\tilde{\alpha} = \alpha + n, \tilde{\lambda} = \lambda
    + \sum_{t = 1}^n x_t).
  \end{equation*}

  La prime bayésienne est donc
  \begin{align*}
    B_{n + 1}
    &= \esp{\Theta^{-1}|S_1, \dots, S_n} \\
    &= \esp{S_{n + 1}|S_1, \dots, S_n} \\
    &= \frac{\tilde{\lambda}}{\tilde{\alpha} - 1} \\
    &= \frac{\lambda + \sum_{t = 1}^n S_t}{\alpha + n - 1} \\
    &= \frac{n}{n + \alpha - 1}\, \bar{S} +
    \frac{\alpha - 1}{n + \alpha - 1} \frac{\lambda}{\alpha - 1} \\
    &= z \bar{S} + (1 - z) m
    \intertext{avec}
    z
    &= \frac{n}{n + \alpha - 1}.
  \end{align*}
  La prime bayésienne est donc linéaire. %
  \qed
\end{exemple}

\begin{exemple}[Normale/normale]
  Soit
  \begin{align*}
    S_t|\Theta
    &\sim \text{Normale}(\Theta, \sigma_2^2) \\
    \Theta
    &\sim \text{Normale}(\mu, \sigma_1^2).
  \end{align*}

  La prime de risque, la variance conditionnelle et la prime
  collective sont:
  \begin{align*}
    \mu(\Theta)
    &= \Theta \\
    \sigma^2(\Theta)
    &= \sigma_2^2 \quad \text{(constante)} \\
    \intertext{et}
    m
    &= \esp{\Theta} = \mu.
  \end{align*}

  Trouver la distribution à postériori n'est toutefois pas une
  sinécure. Tout d'abord,
  \begin{equation*}
    u(\theta|x_1, \dots, x_n) \propto
    e^{-(\theta - \mu)^2/2 \sigma_1^2}\,
    e^{- \sum_{t=1}^n (x_t - \theta)^2/2 \sigma_2^2}.
  \end{equation*}
  En développant l'exposant tout en laissant de côté tous les termes
  non fonction de $\theta$, nous obtenons
  \begin{align*}
    \text{Exposant}
    &= - \frac{1}{2}
      \left[
      \frac{\theta^2 - 2\theta \mu}{\sigma_1^2} +
      \sum_{t=1}^n \frac{-2 \theta x_t + \theta^2}{\sigma_2^2} +
      \text{cte}
      \right] \\
    &= - \frac{1}{2}
      \left[
      \theta^2
      \left(
      \frac{1}{\sigma_1^2} + \frac{n}{\sigma_2^2}
      \right) -
      \frac{2 \theta \mu}{\sigma_1^2} -
      \frac{-2 \theta \sum_{t=1}^n x_t}{\sigma_2^2} +
      \text{cte}
      \right] \displaybreak[0] \\
    &= - \frac{1}{2}
      \underbrace{
      \left(
      \frac{1}{\sigma_1^2} + \frac{n}{\sigma_2^2}
      \right)}_{\displaystyle \phi}
      \left[
      \theta^2 -
      \frac{2 \theta \mu/\sigma_1^2}{(\frac{1}{\sigma_1^2} +
      \frac{n}{\sigma_2^2})} -
      \frac{-2 \theta \sum_{t=1}^n x_t/\sigma_2^2}{(\frac{1}{\sigma_1^2} +
      \frac{n}{\sigma_2^2})} +
      \text{cte}
      \right] \displaybreak[0] \\
    &= - \frac{1}{2} \phi
      \left[
      \theta^2 - 2 \theta
      \left(
      \frac{\mu/\sigma_1^2 + \sum_{t=1}^n x_t/\sigma_2^2}{\phi}
      \right) +
      \text{cte}
      \right] \displaybreak[0] \\
    &= - \frac{1}{2}
      \frac{
      \left(
      \theta -
      \left(
      \frac{\mu}{\phi \sigma_1^2} +
      \frac{\sum_{t=1}^n x_t}{\phi \sigma_2^2}
      \right)
      \right)^2}{1/\phi} +
      \text{cte}.
  \end{align*}
  Par conséquent,
  \begin{equation*}
    \Theta|S_1 = x_1, \dots, S_n = x_n
    \sim \text{Normale}(\tilde{\mu}, \tilde{\sigma}_1^2) \\
  \end{equation*}
  avec
  \begin{align*}
    \tilde{\sigma}_1^2
    &= \frac{1}{\phi} \\
    &= \frac{\sigma_1^2 \sigma_2^2}{\sigma_2^2 + n \sigma_1^2} \\
    &= \frac{\sigma_1^2}{1 + n \sigma_1^2/\sigma_2^2} < \sigma_1^2 \\
    \intertext{et}
    \tilde{\mu}
    &= \frac{\mu}{\phi \sigma_1^2} +
      \frac{\sum_{t = 1}^n x_t}{\phi \sigma_2^2} \\
    &= \frac{\mu \sigma_2^2 + \sigma_1^2 \sum_{t = 1}^n x_t}{%
      \sigma_2^2 + n \sigma_1^2}.
  \end{align*}

  La distribution marginale de $S$ est plus aisée à trouver à l'aide
  des fonctions génératrice des moments. En effet,
  \begin{align*}
    M_S(t)
    &= \esp{\esp{e^{tS}|\Theta}} \\
    &= \esp{e^{\Theta t + \sigma_2^2 t^2/2}} \\
    &= e^{\sigma_2^2 t^2/2} \Esp{e^{\Theta t}} \\
    &= e^{\sigma_2^2 t^2/2} e^{\mu t + \sigma_1^2 t^2/2} \\
    &= e^{\mu t + (\sigma_1^2 + \sigma_2^2) t^2/2}
  \end{align*}
  et donc
  \begin{equation*}
    S \sim \text{Normale}(\mu, \sigma_1^2 + \sigma_2^2).
  \end{equation*}
  Puisque la distribution à postériori de $\Theta$ est du même type
  que la distribution à priori, nous pouvons immédiatement conclure
  que
  \begin{equation*}
    S_{n+1}|S_1, \dots, S_n \sim
    \text{Normale}(\tilde{\mu}, \tilde{\sigma}_1^2 + \tilde{\sigma}_2^2).
  \end{equation*}

  La prime bayésienne est donc, en utilisant indifféremment l'approche
  de la distribution à postériori ou celle de la prédictive,
  \begin{align*}
    B_{n+1}
    &= \tilde{\mu} \\
    &= \frac{n \sigma_1^2}{n \sigma_1^2 + \sigma_2^2}\, \bar{S} +
      \frac{\sigma_2^2}{n \sigma_1^2 + \sigma_2^2} \mu \\
    &= z \bar{S} + (1 - z) m \\
    \intertext{où}
    z
    &= \frac{n}{n + \sigma_2^2/\sigma_1^2}.
  \end{align*}
  \qed
\end{exemple}

Les formules de crédibilité linéaire --- ou exacte --- pour les
combinaisons de distributions issues de la famille exponentielle sont
rassemblées à l'\autoref{chap:formules}. Les commentaires suivants se
rapportent à ces résultats.

\begin{enumerate}
\item Dans la combinaison normale/normale, nous avons
  \begin{equation*}
    \tilde{\sigma}_1^2 =
      \frac{\sigma_1^2}{n \dfrac{\sigma_1^2}{\sigma_2^2} + 1}
    \le \sigma_1^2,
  \end{equation*}
  avec égalité seulement lorsque $\sigma_1^2 = 0$ (le cas $\sigma_2^2
  = \infty$ ne présentant aucun intérêt). Cette inégalité s'interprète
  comme une baisse de l'incertitude quant au niveau de risque d'un
  contrat au fur et à mesure que l'expérience s'accumule.

\item La combinaison normale/normale est celle considérée par
  \citet{Whitney:1918}, mais aussi celle pour laquelle les formules
  sont les plus compliquées. Cela explique sans doute en partie
  pourquoi l'auteur a recommandé de fixer la constante $K$ au
  jugement.

\item Dans tous les cas, $z \rightarrow 1$ lorsque $n \rightarrow
  \infty$.  Le poids accordé à la prime individuelle d'un contrat va
  donc croissant avec le nombre d'années d'expérience disponible.

\item De plus, $z = n/(n + K) \rightarrow 1$ lorsque $K \rightarrow 0$
  et $z \rightarrow 0$ lorsque $K \rightarrow \infty$. Dans la
  combinaison Poisson/gamma, où $K = \lambda$, une petite valeur de
  $\lambda$ correspond à une grande incertitude quant au niveau de
  risque $\theta$ (la courbe gamma sera très évasée, voir la
  \autoref{fig:bayesienne:gamma}). L'assureur accorde donc peu de
  poids à la prime collective, d'où un grand facteur de crédibilité.

\item Dans la combinaison normale/normale, $K$ est grand si
  $\sigma_2^2$ est également grand ou alors si $\sigma_1^2$ est petit.
  Respectivement, cela signifie que ou bien l'expérience est
  potentiellement si volatile que l'on ne peut s'y fier, ou bien que
  le niveau de risque $\theta$ est presque connu avec certitude. Dans
  un cas comme dans l'autre, il convient de charger la prime
  collective. Nous pouvons répéter une telle analyse pour chacune des
  autres combinaisons de distributions.

\item À un haut niveau de risque ne correspond pas nécessairement une
  grande valeur de $\theta$, comme en fait foi la combinaison
  exponentielle/gamma.
\end{enumerate}

\begin{figure}
  \centering
<<echo=FALSE, fig=TRUE>>=
curve(dgamma(x, 3, 3), from=0, to=10, xlab="x", ylab="f(x)")
curve(dgamma(x, 3, 0.5), add=TRUE, lwd=2)
legend(6, 0.8, c("Gamma(3, 3)", "Gamma(3, 1/2)"), lwd=c(1, 2))
@
   \caption{Distributions gamma avec différents paramètres d'échelle
     $\lambda$.}
   \label{fig:bayesienne:gamma}
\end{figure}


\section{Évaluation numérique avec R}
\label{sec:bayesienne:actuar}

Nous abordons ici l'évaluation numérique de primes de crédibilité avec
R, un sujet qui reviendra périodiquement dans les chapitres
subséquents. Nous effectuerons les calculs de crédibilité avec la
fonction \code{cm} du paquetage \pkg{actuar} \citep{actuar}.

Disponible dans le site
\link{https://cran.r-project.org/}{Comprehensive R Archive Network}
(CRAN) depuis 2005 et toujours en développement, \pkg{actuar} ajoute
au système R de base des fonctionnalités spécifiques pour les
applications actuarielles: modélisation des distributions de
sinistres; théorie du risque et de la ruine; simulation de mélanges et
de modèles composés hiérarchiques; théorie de la crédibilité. Le
paquetage fournit également des fonctions pour le traitement de 19
lois de probabilité continues à valeurs extrêmes et les versions zéro
tronquée et zéro modifiée des lois discrètes usuelles. Nous vous
encourageons vivement à consulter l'exhaustive documentation du
paquetage sous forme de \emph{vignettes}. La commande ci-dessous
permet d'afficher la liste des vignettes disponibles.
<<echo=TRUE, eval=FALSE>>=
vignette(package = "actuar")
@

La fonction \code{cm} de \pkg{actuar} sert d'interface unique pour
l'ajustement de plusieurs modèles de crédibilité de précision: les
modèles bayésiens linéaires de la section précédente, les modèles de
\citet{Buhlmann:credibility:1969} et de \citet{BuhlmannStraub:1970},
le modèle hiérarchique de \citet{Jewell:hierarchical:1975} et le
modèle de régression de \citet{Hachemeister:1975}. L'interface de la
fonction est fortement inspirée de \code{lm}, la fonction d'ajustement
de modèles linéaires de R.

Pour la tarification bayésienne pure telle qu'étudiée dans le présent
chapitre, la fonction \code{cm} prend en charge toutes les
combinaisons de distributions mentionnées à la
\autoref{sec:bayesienne:lineaire}, les convolutions binomiale/bêta
(\autoref{exercice:bayesienne:binomiale-beta}), gamma/gamma
(\autoref{exercice:bayesienne:gamma-gamma}) et binomiale
négative/bêta, ainsi que la combinaison quelque peu périphérique Pareto
translatée/gamma (\autoref{exercice:bayesienne:pareto-gamma}).

Nous décrivons l'utilisation de la fonction pour le calcul de primes
bayésiennes linéaires à même le code informatique de la
\autoref{sec:bayesienne:code}.


\section{Modèle de Jewell}
\label{sec:bayesienne:jewell}

Le modèle de crédibilité exacte de Jewell unifie les résultats des
cinq cas spéciaux étudiés précédemment. Nous n'en traçons que les
grandes lignes ici.

\begin{itemize}
\item En analyse bayésienne, si $u(\theta|x_1, \dots, x_n)$ appartient
  à la même famille que $u(\theta)$, on dit de $u(\theta)$ et
  $f(x|\theta)$ qu'elles sont des \emph{conjuguées naturelles}.
\item Les lois de Poisson, exponentielle, normale, Bernoulli et
  géométrique appartiennent toutes à la \emph{famille exponentielle
    univariée}, c'est-à-dire que leur fonction de densité (ou de
  probabilité) peut s'écrire sous la forme
  \begin{equation*}
    f(x|\theta) = \frac{p(x) e^{-\theta x}}{q(\theta)}.
  \end{equation*}
\item \citet{Jewell:exact:1974} démontre que lorsqu'une fonction de
  vraisemblance de la famille exponentielle est combinée avec sa
  conjuguée naturelle, alors la prime bayésienne est toujours une
  prime de crédibilité exacte.
\item \citet{Goel:conjecture:1982} conjecture que ceci n'arrive
  qu'avec les membres de la famille exponentielle, c'est-à-dire qu'il
  ne peut le prouver, mais qu'il ne peut non plus fournir de
  contre-exemple.
\end{itemize}


\section{Code informatique}
\label{sec:bayesienne:code}

\def\scriptfilename{bayesienne.R}

\scriptfile{\scriptfilename}
\lstinputlisting[firstline=14]{\scriptfilename}


\section{Exercices}
\label{sec:bayesienne:exercices}

\Opensolutionfile{reponses}[reponses-bayesienne]
\Opensolutionfile{solutions}[solutions-bayesienne]

\begin{Filesave}{solutions}
\section*{Chapitre \ref*{chap:bayesienne}}
\addcontentsline{toc}{section}{Chapitre \protect\ref*{chap:bayesienne}}

\end{Filesave}

%%%
%%% Bayésien pur
%%%

\begin{exercice}
  Un portefeuille d'assurance automobile est composé de $35$~\% de bons
  conducteurs, $40$~\% de conducteurs moyens et $25$~\% de mauvais
  conducteurs. L'actuaire a estimé que les bons conducteurs ont, en
  moyenne, un accident par 10 ans, les conducteurs moyens, deux
  accidents et les mauvais conducteurs, six accidents. L'actuaire
  suppose de plus que la fréquence des accidents a une distribution de
  Poisson. Par souci de simplicité, les sinistres sont tous d'un
  montant de 1.
  \begin{enumerate}
  \item Quelle est la probabilité qu'un assuré choisi au hasard ait un
    accident?
  \item Calculer la prime de risque pour chacun des trois types de
    conducteurs.
  \item Calculer la prime collective.
  \item Calculer la prime bayésienne de sixième année d'un contrat
    ayant le dossier suivant au cours des cinq premières années: 1, 0,
    1, 1, 0.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item 0,1795
    \item 0,1, 0,2 et 0,6
    \item 0,265
    \item 0,4585
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    Soit $S$ le montant total des sinistres (ou le nombre de
    sinistres, car chaque sinistre est d'un montant de 1).  Alors
    $S|\Theta \sim \text{Poisson}(\Theta)$ et $\Theta$ a une fonction
    de masse de probabilité
    \begin{equation*}
      \Pr[\Theta = \theta] =
      \begin{cases}
        0,35, & \theta = 1/10 = 0,1 \\
        0,40, & \theta = 2/10 = 0,2 \\
        0,25, & \theta = 6/10 = 0,6.
      \end{cases}
    \end{equation*}
    \begin{enumerate}
    \item Par la loi des probabilités totales,
      \begin{align*}
        \Pr[S = 1] &= \sum_{\theta} \Pr[S=1|\Theta = \theta]
        \Pr[\Theta = \theta] \\
        &= (0,1\, e^{-0,1}) (0,35) + (0,2\, e^{-0,2}) (0,40) +
        (0,6\, e^{-0,6}) (0,25) \\
        &= 0,1795.
      \end{align*}
    \item Ici, $\mu(\theta) = \esp{S|\Theta = \theta} = \theta$. Nous
      avons donc $\mu(0,1) = 0,1$, $\mu(0,2) = 0,2$ et
      $\mu(0,6) = 0,6$.
    \item $\esp{S} = \esp{\mu(\Theta)} = \esp{\Theta} = 0,35 (0,1) +
      0,40 (0,2) + 0,25(0,6) = 0,265$.
    \item La première étape consiste à calculer la fonction de masse
      de probabilité à postériori de $\Theta$. Soit $\mat{S} = (S_1,
      S_2, S_3, S_4, S_5)$ et $\mat{x} = (1, 0, 1, 1, 0)$. Alors, par
      l'indépendance conditionnelle de $S_1, \dots, S_5$,
      \begin{equation*}
        \Pr[\mat{S} = \mat{x}|\Theta = \theta]
        = \prod_{j=1}^5 \Pr[S_j = x_j|\Theta = \theta]
        = \theta^3 e^{-5 \theta}.
      \end{equation*}
      Nous obtenons donc $\Pr[\mat{S} = \mat{x}|\Theta = 0,1] =
      0,000607$, $\Pr[\mat{S} = \mat{x}|\Theta = 0,2] = 0,002943$,
      $\Pr[\mat{S} = \mat{x}|\Theta = 0,6] = 0,010754$ et, par la loi
      des probabilités totales, $\Pr[\mat{S} = \mat{x}] =
      0,004078$. En utilisant la règle de Bayes,
      \begin{equation*}
        \Pr[\Theta = \theta|\mat{S} = \mat{x}] =
        \begin{cases}
          0,0521, & \theta = 0,1 \\
          0,2887, & \theta = 0,2 \\
          0,6592, & \theta = 0,6
        \end{cases}
      \end{equation*}
      et, finalement, la prime bayésienne de sixième année est
      $\esp{\Theta|\mat{S} = \mat{x}} = 0,0521(0,1) + 0,2887(0,2) +
      0,6592(0,6) = 0,4585$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Le \emph{Zchoulp} se joue avec deux dés à six faces.  Le premier est
  un dé usuel, dont les faces sont numérotées de 1 à 6. Le second dé a
  deux faces numérotées 5 et les autres numérotées de 1 à 4.
  \begin{enumerate}
  \item Un dé est choisi au hasard et lancé. Quelle est la probabilité
    d'obtenir un 5?
  \item Si le résultat du lancer en a) est un 5 et que le même dé est
    relancé, quelle est la probabilité d'obtenir à nouveau un 5?
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $1/4$
    \item $5/18$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    Soit $S_i$ le résultat du $i${\ieme} lancer d'un dé, $D_1$
    l'événement «choisir le dé régulier» et $D_2$ l'événement «choisir
    le dé irrégulier».
    \begin{enumerate}
    \item Par la loi des probabilités totales,
      \begin{align*}
        \Pr[S_1 = 5]
        &= \Pr[S_1 = 5|D_1] \Pr[D_1] + \Pr[S_1 = 5|D_2] \Pr[D_2] \\
        &= \left(\frac{1}{6}\right) \left(\frac{1}{2}\right)
        + \left(\frac{2}{6}\right) \left(\frac{1}{2}\right) \\
        &= \frac{1}{4}.
      \end{align*}
      Plus simplement, nous avons trois faces numérotées 5 sur un
      total de douze, d'où une probabilité de $\frac{1}{4}$.
    \item Nous cherchons la valeur de la fonction de masse de
      probabilité de la distribution prédictive de $S_2$ au point
      $x = 5$:
      \begin{align*}
        \Pr[S_2 = 5|S_1 = 5]
        &= \frac{1}{\Pr[S_1 = 5]}
        \left(
          \Pr[S_1 = 5, S_2 = 5|D_1] \Pr[D_1]
        \right. \\
        & \quad\; +
        \left.
          \Pr[S_1 = 5, S_2 = 5|D_2] \Pr[D_2]
        \right) \\
        &= \frac{(\frac{1}{36}) (\frac{1}{2})
          + (\frac{4}{36}) (\frac{1}{2})}{\frac{1}{4}} \\
        &= \frac{5}{18}.
      \end{align*}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Deux urnes contiennent chacune une pièce de monnaie. La pièce dans
  l'urne A tombe sur face $40$~\% des fois et celle dans l'urne B, $80$~\%
  des fois. Nous choisissons une urne au hasard (la probabilité de choisir
  l'urne A est la même que celle de choisir l'urne B) et nous prenons la
  pièce qui s'y trouve. Nous lançons la pièce en l'air cinq fois et elle
  retombe sur pile quatre fois. Si nous lançons la pièce à cinq autres
  reprises, quel est le nombre espéré de fois que la pièce retombera
  sur pile?
  \begin{rep}
    $2,95$
  \end{rep}
  \begin{sol}
    Soit $S$ le nombre de piles obtenus après $n$ lancers d'une pièce
    de monnaie et $\Theta$ l'urne choisie. Nous identifions l'urne A
    par $\theta = 0,6$ et l'urne B par $\theta = 0,2$. Ainsi,
    $S|\Theta = \theta \sim \text{Binomiale}(n, \theta)$. Nous devons
    calculer $\esp{S_2|S_1 = 4}$. La distribution à
    postériori de $\Theta|S_1 = 4$ est
    \begin{equation*}
      \Pr[\Theta = \theta|S_1 = 4]
      = \frac{\Pr[S_1 = 4|\Theta = \theta] \Pr[\Theta = \theta]}{%
        \sum_\theta \Pr[S_1 = 4|\Theta = \theta] \Pr[\Theta = \theta]}.
    \end{equation*}
    Par conséquent,
    \begin{align*}
      \Pr[\Theta = 0,6|S_1 = 4]
      &= \frac{\binom{5}{4} (0,6)^4 (0,4) (0,5)}{%
        \binom{5}{4} (0,6)^4 (0,4) (0,5) +
        \binom{5}{4} (0,2)^4 (0,8) (0,5)} \\
      &= \frac{0,1296}{0,1328} \\
      &= 0,9759.
    \end{align*}
    De la même façon,
    $\Pr[\Theta = 0,2|S_1 = 4] = 0,0032/0,1328 = 0,02410$. Enfin,
    \begin{align*}
      \esp{S_2|S_1 = 4}
      &= \esp{S_2|\theta = 0,6} \Pr[\Theta = 0,6|S_1 = 4] \\
      &\phantom{=}
      + \esp{S_2|\Theta = 0,2} \Pr[\Theta = 0,2|S_1 = 4] \\
      &= 5 (0,6) (0,9759) + 5 (0,2) (0,02410) \\
      &= 2,9518.
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  Les employeurs couverts par le régime d'assurance médicament d'un
  assureur sont classés par ce dernier dans trois groupes de taille
  égale. La probabilité de subir un sinistre dans une période pour
  chacun de ces groupes est donnée dans le tableau ci-dessous.
  \begin{center}
    \begin{tabular}{lc}
      \toprule
      Groupe  & Probabilité de sinistre \\
      \midrule
      Fréquence faible    & $10$~\% \\
      Fréquence moyenne   & $20$~\% \\
      Fréquence élevée    & $40$~\% \\
      \bottomrule
    \end{tabular}
  \end{center}
  L'assureur suppose de plus que les sinistres sont indépendants à
  l'intérieur de chacun des groupes.
  \begin{enumerate}
  \item Quelle est la probabilité qu'un employeur choisi au hasard
    dans ce portefeuille ait un sinistre?
  \item Après deux années d'expérience, l'employeur choisi en a)
    présente un dossier de sinistre vierge. À la lumière de ces
    résultats, quelle est la probabilité que cet employeur fasse
    partie du groupe à fréquence de sinistre faible?
  \item Quelle est maintenant la probabilité que l'employeur mentionné
    ci-dessus ait un sinistre lors de la troisième année?
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item 7/30
    \item 0,4475
    \item 0,1950
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    Soit $S$ une variable indiquant si un employeur a eu un sinistre
    dans une année et $\Theta$ une variable identifiant le groupe
    auquel cet employeur appartient. Identifions le groupe à fréquence
    faible par $\theta = 0,1$, le groupe à fréquence moyenne par
    $\theta = 0,2$ et le groupe à fréquence élevée par $\theta = 0,4$.
    Nous avons donc $S|\Theta = \theta \sim \text{Bernoulli}(\theta)$.
    \begin{enumerate}
    \item Par la loi des probabilités totales,
      \begin{align*}
        \Pr[S = 1]
        &= \sum_\theta \Pr[S = 1|\Theta = \theta] \Pr[\Theta = \theta] \\
        &= (0,1 + 0,2 + 0,4) \left(\frac{1}{3}\right) \\
        &= \frac{7}{30}.
      \end{align*}
    \item En utilisant la règle de Bayes et l'hypothèse d'indépendance
      des sinistres,
      \begin{align*}
        \Pr[\Theta = 0,1|S_1 = 0, S_2 = 0]
        &= \frac{\Pr[S_1 = 0, S_2 = 0|\Theta = 0,1] \Pr[\Theta = 0,1]}{%
          \Pr[S_1 = 0, S_2 = 0]} \\
        &= \frac{(0,9)^2 (\frac{1}{3})}{%
          (0,9)^2 (\frac{1}{3}) +
          (0,8)^2 (\frac{1}{3}) +
          (0,6)^2 (\frac{1}{3})} \\
        &= \frac{81}{181} \\
        &= 0,4475.
      \end{align*}
    \item De la même façon qu'en b), nous trouvons
      $\Pr[\Theta = 0,2|S_1 = 0, S_2 = 0] = 64/181 = 0,3536$ et
      $\Pr[\Theta = 0,4|S_1 = 0, S_2 = 0] = 36/181 = 0,1989$. Ainsi,
      \begin{align*}
        \Pr[S_3 = 1|S_1 = 0&, S_2 = 0] \\
        &= \sum_\theta \Pr[S_3 = 1|\Theta = \theta] \Pr[\Theta = \theta|S_1 = 0, S_2 = 0] \\
        &= \left(\frac{1}{181}\right)[(0,1)(81) + (0,2)(64) + (0,4)(36)] \\
        &= \frac{35,3}{181} \\
        &= 0,1950.
      \end{align*}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Nous souhaitons vérifier si une pièce de monnaie est équilibrée ou
  non (probabilité de $\frac{1}{2}$ de tomber sur pile ou face).
  L'incertitude quant à la probabilité $\theta$ d'obtenir, disons,
  pile lors d'un lancer de la pièce est traduite en une variable
  aléatoire $\Theta$. Celle-ci est distribuée selon une loi Bêta de
  paramètres $a > 0$ et $b > 0$.
  \begin{enumerate}
  \item Trouver $\esp{\Theta}$.
  \item La pièce de monnaie est lancée $n$ fois. La variable aléatoire
    $S$ représente le nombre de fois que la pièce est tombée sur face.
    Trouver la distribution à postériori de $\Theta$.
  \item Une petite expérience pratique maintenant, aussi ludique
    qu'enrichissante. Jouez à pile ou face une bonne dizaine de fois
    avec une pièce quelconque. Enregistrer une valeur de $0$ pour pile
    et de $1$ pour face. Après chaque lancer, calculez la distribution
    à postériori de $\Theta$ et tracez-en un graphique approximatif.
    Vous pouvez aussi utiliser les fonctions \code{curve} et
    \code{dbeta} de R pour faire les graphiques. Par exemple, pour
    tracer la densité d'une bêta avec paramètres $a = 4$ et $b = 2$
    l'expression à utiliser est
<<echo=TRUE, eval=FALSE>>=
curve(dbeta(x, 4, 2), from = 0, to = 1)
@
    Observez les déplacements de la densité en fonction des résultats.
    Commencez l'expérience avec $a = b = 1$, soit $\Theta
    \sim U(0, 1)$.
  \item Quelle est votre estimation de la probabilité d'obtenir pile
    au onzième lancer de la pièce si vous ignorez les résultats des
    dix premiers lancers?
  \item Quelle est maintenant votre estimation si vous connaissez les
    résultats des dix lancers effectués en c)?
  \end{enumerate}
  \begin{rep}
    \begin{enumerate}
    \item $a/(a + b)$
    \item $\Theta|S = x \sim \text{Bêta}(a + x, b + n - x)$
      \stepcounter{enumi}
    \item $a/(a + b)$
    \item $(a + x)/(a + b + n)$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Nous avons
      \begin{align*}
        \esp{\Theta}
        &= \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)}
        \int_0^1 \theta^{a - 1} (1 - \theta)^{b - 1}\, d\theta \\
        &= \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)}
        \frac{\Gamma(a+1) \Gamma(b)}{\Gamma(a+b+1)} \\
        &= \frac{a}{a + b}.
      \end{align*}
    \item Nous avons $S|\Theta = \theta \sim \text{Binomiale}(n, \theta)$,
      c'est-à-dire $\Pr[S = x|\Theta = \theta] = \binom{n}{x}\theta^x
      (1 - \theta)^{n-x}$. Par la règle de Bayes,
      \begin{align*}
        u(\theta|x)
        &\propto \theta^x (1 - \theta)^{n-x}
        \theta^{a - 1} (1 - \theta)^{b - 1} \\
        &= \theta^{a + x - 1}(1 - \theta)^{b + n - x - 1},
      \end{align*}
      que nous savons être la densité d'une bêta avec de nouveaux
      paramètres.  Donc, $\Theta|S = x \sim \text{Bêta}(\tilde{a}
      = a + x, \tilde{b} = b + n - x)$.
    \item Une pièce a été lancée dix fois et les résultats suivants
      furent obtenus: P, F, F, P, F, P, F, F, P, P. En utilisant les
      résultats obtenus précédemment, la distribution à postériori de
      $\Theta$ est une bêta et les paramètres $(a, b)$ après chaque
      lancer sont les suivants, dans l'ordre: $(1, 2)$, $(2, 2)$,
      $(3, 2)$, $(3, 3)$, $(4, 3)$, $(4, 4)$, $(5, 4)$, $(6, 4)$,
      $(6, 5)$, $(6, 6)$. Les graphiques de quelques distributions à postériori sont
      présentés aux figures
      \ref{fig:bayesienne:beta0}--\ref{fig:bayesienne:beta5}. Une
      première observation est le déplacement vers la gauche ou vers
      la droite de la distribution selon le nombre de résultats piles
      et de résultats faces obtenus. Une deuxième observation est la
      concentration graduelle de la distribution autour de
      $\theta = \frac{1}{2}$, ce qui indique une pièce équilibrée.
      \begin{figure}
        \begin{minipage}[t]{0.45\textwidth}
<<echo=FALSE, fig=TRUE, height=4, width=4.5>>=
par(mar = c(2, 4, 2, 0))
curve(dbeta(x, 1, 1), xlim = c(0, 1))
@
          \caption{Distribution à priori (uniforme)}
          \label{fig:bayesienne:beta0}
        \end{minipage}
        \hfill
        \begin{minipage}[t]{0.45\textwidth}
<<echo=FALSE, fig=TRUE, height=4, width=4.5>>=
par(mar = c(2, 4, 2, 0))
curve(dbeta(x, 1, 2), xlim = c(0, 1))
@
          \caption{Distribution après un lancer (P)}
        \end{minipage}
        \begin{minipage}[t]{0.45\textwidth}
<<echo=FALSE, fig=TRUE, height=4, width=4.5>>=
par(mar = c(2, 4, 2, 0))
curve(dbeta(x, 2, 2), xlim = c(0, 1))
@
          \caption{Distribution après deux lancers (P, F)}
        \end{minipage}
        \hfill
        \begin{minipage}[t]{0.45\textwidth}
<<echo=FALSE, fig=TRUE, height=4, width=4.5>>=
par(mar = c(2, 4, 2, 0))
curve(dbeta(x, 3, 2), xlim = c(0, 1))
@
          \caption{Distribution après trois lancers (P, F, F)}
        \end{minipage}
        \begin{minipage}[t]{0.45\textwidth}
<<echo=FALSE, fig=TRUE, height=4, width=4.5>>=
par(mar = c(2, 4, 2, 0))
curve(dbeta(x, 3, 3), xlim = c(0, 1))
@
          \caption{Distribution après quatre lancers (P, F, F, P)}
        \end{minipage}
        \hfill
        \begin{minipage}[t]{0.45\textwidth}
<<echo=FALSE, fig=TRUE, height=4, width=4.5>>=
par(mar = c(2, 4, 2, 0))
curve(dbeta(x, 4, 3), xlim = c(0, 1))
@
          \caption{Distribution après cinq lancers (P, F, F, P, F)}
          \label{fig:bayesienne:beta5}
        \end{minipage}
      \end{figure}
    \item Soit $Y$ le résultat du onzième lancer, où $Y = 1$
      correspond à face. Nous avons
      \begin{align*}
        \Pr[Y = 1]
        &= \int_0^1 \Pr[Y = 1|\Theta = \theta]
        \Pr[\Theta = \theta]\, d\theta \\
        &= \int_0^1 \theta \Pr[\Theta = \theta]\, d\theta \\
        &= \esp{\Theta} \\
        &= \frac{a}{a + b}.
      \end{align*}
      Le résultat n'est pas nécessairement $\frac{1}{2}$ puisque nous
      ignorons si la pièce est équilibrée ou non.
    \item Nous avons
      \begin{align*}
        \Pr[Y = 1|S = x]
        &= \int_0^1 \Pr[Y = 1|\Theta = \theta]
        \Pr[\Theta = \theta|S = x]\, d\theta \\
        &= \int_0^1 \theta \Pr[\Theta = \theta|S = x]\, d\theta \\
        &= \esp{\Theta|S = x} \\
        &= \frac{a + x}{a + b + n}.
      \end{align*}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Votre opinion à priori quant à la distribution du montant des
  sinistres est une loi de Pareto de paramètres $\lambda = 10$ et
  $\theta = 1, 2$ ou $3$, ces valeurs de $\theta$ étant toutes
  équiprobables. Pour un contrat choisi au hasard, vous observez par
  la suite un sinistre d'un montant de $20$. Déterminer la probabilité
  que le montant du prochain sinistre de ce contrat soit supérieur à
  $30$.
  \begin{rep}
    $0,1484$
  \end{rep}
  \begin{sol}
    Nous avons le modèle suivant:
    $X|\Theta \sim \text{Pareto}(\Theta, 10)$ et
    $\Theta \sim U(1, 2, 3)$. La fonction de répartition de
    $X|\Theta = \theta$ est
    \begin{equation*}
      F(x|\theta) = 1 -
      \left(
        \frac{\lambda}{\lambda + x}
      \right)^\theta.
    \end{equation*}
    Ainsi,
    \begin{align*}
      \Pr[X_2 > 30|X_1 = 20]
      &= \sum_{\theta=1}^3
      \Pr[X_2 > 30|\Theta = \theta] \Pr[\Theta = \theta|X_1 = 20] \\
      &= \sum_{\theta=1}^3 (1 - F(30|\theta))
      \frac{f(20|\theta) (\frac{1}{3})}{%
        \sum_{\theta = 1}^3 f(20|\theta) (\frac{1}{3})} \\
      &= \frac{ \sum_{\theta=1}^3 \left(\frac{10}{10 + 30}\right)^\theta
        \frac{\theta 10^\theta}{(10 + 20)^{\theta+1}}}{
        \sum_{\theta=1}^3
        \frac{\theta 10^\theta}{(10 + 20)^{\theta+1}}} \\
      &= 0,1484.
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  On demande à Camille d'élaborer un modèle pour la fréquence des
  sinistres au sein d'un portefeuille composé de dix contrats.
  Incertaine quant à la probabilité d'avoir un accident, Camille
  estime à $20$~\% la possibilité que la probabilité soit de $0,04$,
  $60$~\% qu'elle soit de $0,10$ et $20$~\% qu'elle soit de $0,16$.
  \begin{enumerate}
  \item Quel est le modèle de Camille pour $N$, le nombre total
    d'accidents du portefeuille au cours d'une année?
  \item Quelle est la probabilité qu'il y ait $0$, $1$ et $2$
    accidents au cours d'une année?
  \item Comparer ces résultats avec la situation où Camille serait
    certaine que la probabilité d'accident est de $0,10$.
  \end{enumerate}
  \begin{rep}
    \begin{enumerate}
      \stepcounter{enumi}
    \item $\Pr[N = 0] = 0,377$, $\Pr[N = 1] = 0,354$, $\Pr[N = 2] =
      0,184$
    \item $\Pr[N = 0|\Theta = 0,10] = 0,349$, $\Pr[N = 1|\Theta =
      0,10] = 0,387$, $\Pr[N = 2|\Theta = 0,10] = 0,194$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item $N|\Theta = \theta \sim \text{Binomiale}(10, \theta)$ et la
      fonction de masse de probabilité de $\Theta$ est
      \begin{equation*}
        \Pr[\Theta = \theta] =
        \begin{cases}
          0,2, & \theta = 0,04 \\
          0,6, & \theta = 0,10 \\
          0,2, & \theta = 0,16.
        \end{cases}
      \end{equation*}
    \item En général, nous avons
      \begin{align*}
        \Pr[N = n] &= \sum_\theta \Pr[N = n|\Theta = \theta]
        \Pr[\Theta = \theta] \\
        &= \sum_\theta \binom{10}{n} \theta^n (1 - \theta)^{10-n}
        \Pr[\Theta = \theta],
      \end{align*}
      ce qui donne $\Pr[N = 0] = 0,377$, $\Pr[N = 1] = 0,354$ et
      $\Pr[N = 2] = 0,184$.
    \item Nous réutilisons la formule en b) avec
      $\Pr[\Theta = 0,10] = 1$. Les résultats sont:
      $\Pr[N = 0] = 0,349$, $\Pr[N = 1] = 0,387$ et
      $\Pr[N = 2] = 0,194$.
    \end{enumerate}
  \end{sol}
\end{exercice}

%%%
%%% Crédibilité bayésienne
%%%

\begin{exercice}
  Soit $S|\Theta \sim \text{Gamma}(2, \Theta)$ et $\Theta^{-1} \sim
  \text{Bêta}(2, 1)$. Trouver $\var{S}$.
  \begin{rep}
    $11/9$
  \end{rep}
  \begin{sol}
    Soit $S|\Theta \sim \text{Gamma}(\tau, \Theta)$ et $\Theta^{-1} \sim
    \text{Bêta}(\alpha, \beta)$. Alors $\esp{S|\Theta} = \tau\,
    \Theta^{-1}$ et $\var{S|\Theta} = \tau\, \Theta^{-2}$, d'où
    \begin{align*}
      \var{S}
      &= \var{\esp{S|\Theta}} + \esp{\var{S|\Theta}} \\
      &= \tau^2 \var{\Theta^{-1}} + \tau \esp{\Theta^{-2}} \\
      &= \frac{\tau^2 \alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)} +
         \frac{\tau \alpha (\alpha+1)}{(\alpha+\beta)(\alpha+\beta+1)}.
     \end{align*}
     Avec $\tau = 2$, $\alpha = 2$ et $\beta = 1$, $\var{S} = 11/9$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $S$ la variable aléatoire représentant le nombre de sinistres
  d'un contrat d'assurance au cours d'une année. Le nombre de
  sinistres a une distribution de Poisson de paramètre inconnu
  $\Theta$. La fonction de densité de probabilité de $\Theta$ est la
  suivante:
  \begin{equation*}
    u(\theta) = \frac{5}{4} \frac{1}{\theta^2}, \quad 1 < \theta < 5.
  \end{equation*}
  \begin{enumerate}
  \item Calculer $\Pr[S = 2]$.
  \item Calculer $\Pr[S_3 = 0|S_1 = 1, S_2 = 1]$.
  \item Calculer la prime bayésienne de troisième année étant donné
    l'expérience en b).
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $0,2257$
    \item $0,2453$
    \item $1,4987$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    Nous avons
    \begin{align*}
      \Pr[S = x|\Theta = \theta]
      &= \frac{\theta^x e^{-\theta}}{x!}, \quad x = 0, 1, \dots, \\
      u(\theta)
      &= \left(\frac{5}{4}\right) \left(\frac{1}{\theta^2}\right),
      \quad 1 < \theta < 5.
    \end{align*}
    \begin{enumerate}
    \item Par la loi des probabilités totales,
      \begin{align*}
        \Pr[S = 2]
        &= \int_1^5 \Pr[S = 2|\Theta = \theta] u(\theta)\, d\theta \\
        &= \frac{5}{4} \int_1^5 \frac{\theta^2 e^{-\theta}}{2!\,
          \theta^2}\, d\theta \\
        &= \left(\frac{5}{8}\right) (e^{-1} - e^{-5}) \\
        &= 0,2257.
      \end{align*}
    \item Déterminons d'abord la densité à postériori de $\Theta$.
      Nous avons:
      \begin{align*}
        u(\theta|x_1=1, x_2=1)
        &= \frac{\Pr[S_1 = 1|\Theta = \theta]
          \Pr[S_2 = 1|\Theta = \theta] u(\theta)}{%
          \int_1^5 \Pr[S_1 = 1|\Theta = \theta]
          \Pr[S_2 = 1|\Theta = \theta] u(\theta)} \\
        &= \frac{e^{-2 \theta}}{\int_1^5 e^{-2 \theta}\, d\theta} \\
        &= \frac{2 e^{-2 \theta}}{e^{-2} - e^{-10}} \\
        &= 14,7831 e^{-2\theta}.
      \end{align*}
      Par conséquent,
      \begin{align*}
        \Pr[S_3 = 0|S_1 = 1, S_2 = 1]
        &= \int_1^5 \Pr[S_3 = 0|\Theta = \theta]
        u(\theta|x_1=1, x_2=1)\, d\theta \\
        &= \frac{2}{e^{-2} - e^{-10}}
        \int_1^5 e^{-3 \theta}\, d\theta \\
        &= \frac{2}{3}\, \frac{e^{-3} - e^{-15}}{e^{-2} - e^{-10}} \\
        &= 0,2453.
      \end{align*}
    \item Avant tout, il faut noter que $\mu(\Theta) = \Theta$. Puis,
      en intégrant par parties,
      \begin{align*}
        \esp{\Theta|S_1 = 1, S_2 = 1}
        &= \int_1^5 u(\theta|x_1 = 1, x_2 = 1)\, d\theta \\
        &= \frac{2}{e^{-2} - e^{-10}}
        \int_1^5 \theta e^{-2 \theta}\, d\theta \\
        &= \frac{2}{e^{-2} - e^{-10}}
        \left[
          -\frac{\theta}{2} e^{-2\theta} - \frac{1}{4} e^{-2 \theta}
        \right|_1^5 \\
        &= \frac{2}{e^{-2} - e^{-10}}
        \left(
          \frac{3}{4} e^{-2} - \frac{11}{4} e^{-10}
        \right) \\
        &= 1,4987.
      \end{align*}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{exercice:bayesienne:YARD}
  La compagnie YARD assure un groupe de maisons contre les incendies.
  Son actuaire a divisé les maisons en trois classes de risque
  équiprobables: A, B et C. La probabilité qu'une maison prenne feu
  dans une année est de $\frac{1}{4}$, quelle que soit la classe. La
  distribution du montant à payer sachant qu'il y a eu incendie est
  donnée dans le tableau ci-dessous.  Quelle est la prime bayésienne
  pour la deuxième année pour un assuré qui a eu un sinistre de
  $\nombre{30000}$~\$ l'année dernière?
  \begin{center}
    \begin{tabular}{cccc}
      \toprule
      & \multicolumn{3}{c}{Probabilité} \\
      \cmidrule{2-4}
      Montant du sinistre & Classe A & Classe B & Classe C \\
      \midrule
      $\nombre{10000}$ & $3/5$ & $0$   & $1/5$ \\
      $\nombre{20000}$ & $1/5$ & $1/2$ & $1/5$ \\
      $\nombre{30000}$ & $1/5$ & $1/2$ & $3/5$ \\
      \bottomrule
    \end{tabular}
  \end{center}
  \begin{rep}
    $\nombre{5788}$
  \end{rep}
  \begin{sol}
    Dans ce problème, il faut réaliser qu'il y a une probabilité de
    $3/4$ de ne pas avoir de sinistre ou, de manière équivalente, un
    sinistre de montant $0$. Définissons les variables aléatoires $S$
    représentant le montant d'un sinistre dans une année et $\Theta$
    identifiant la classe de risque. Par abus de notation, nous allons
    considérer que les valeurs possibles de $\Theta$ sont
    $\theta = A$, $B$ et $C$. À partir des données du tableau, nous
    pouvons construire la fonction de masse de probabilité de la
    variable aléatoire conditionnelle $S|\Theta = \theta$:
    \begin{align*}
      \Pr[S = x|\Theta = A]
      &=
      \begin{cases}
        \frac{3}{4}, & x = 0 \\
        (\frac{1}{4}) (\frac{3}{5}) = \frac{3}{20}, & x = \nombre{10000} \\
        (\frac{1}{4}) (\frac{1}{5}) = \frac{1}{20}, & x = \nombre{20000} \\
        (\frac{1}{4}) (\frac{1}{5}) = \frac{1}{20}, & x = \nombre{30000} \\
      \end{cases} \\
      \Pr[S = x|\Theta = B]
      &=
      \begin{cases}
        \frac{3}{4}, & x = 0 \\
        0, & x = \nombre{10000} \\
        (\frac{1}{4}) (\frac{1}{2}) = \frac{1}{8}, & x = \nombre{20000} \\
        (\frac{1}{4}) (\frac{1}{2}) = \frac{1}{8}, & x = \nombre{30000} \\
      \end{cases} \\
      \Pr[S = x|\Theta = A]
      &=
      \begin{cases}
        \frac{3}{4}, & x = 0 \\
        (\frac{1}{4}) (\frac{1}{5}) = \frac{1}{20}, & x = \nombre{10000} \\
        (\frac{1}{4}) (\frac{1}{5}) = \frac{1}{20}, & x = \nombre{20000} \\
        (\frac{1}{4}) (\frac{3}{5}) = \frac{3}{20}, & x = \nombre{30000} \\
      \end{cases}
    \end{align*}
    et $\Pr[\Theta = \theta] = 1/3$, $\theta = A, B, C$. Pour la
    suite, il y a deux solutions possibles.
    \begin{enumerate}[1.]
    \item Avec la distribution à postériori de $\Theta$. Tout d'abord,
      nous avons
      \begin{align*}
        \Pr[\Theta = \theta|X_1 = \nombre{30000}]
        &= \frac{\Pr[X_1 = \nombre{30000}|\Theta = \theta]\Pr[\Theta =
          \theta]}%
        {\sum_{\theta}\Pr[X_1 = \nombre{30000}|\Theta =
          \theta]\Pr[\Theta = \theta]} \\
        &=
        \begin{cases}
          \frac{\frac{1}{20}}%
          {\frac{1}{20} + \frac{1}{8} + \frac{3}{20}} = \frac{2}{13},
          & \theta = A \\
          \frac{\frac{1}{8}}%
          {\frac{1}{20} + \frac{1}{8} + \frac{3}{20}} = \frac{5}{13},
          & \theta = B \\
          \frac{\frac{3}{20}}%
          {\frac{1}{20} + \frac{1}{8} + \frac{3}{20}} = \frac{6}{13},
          & \theta = C.
        \end{cases}
      \end{align*}
      Or,
      \begin{align*}
        \mu(A) = \esp{X|\Theta = A} &= \nombre{4000} \\
        \mu(B) = \esp{X|\Theta = B} &= \nombre{6250} \\
        \mu(C) = \esp{X|\Theta = C} &= \nombre{6000},
      \end{align*}
      d'où la prime bayésienne pour la deuxième année est
      \begin{align*}
        \esp{\mu(\Theta)|X_1 = \nombre{30000}}
        &= \nombre{4000} \left( \frac{2}{13} \right)
        + \nombre{6250} \left( \frac{5}{13} \right)
        + \nombre{6000} \left( \frac{6}{13} \right) \\
        &=\nombre{5788,46}.
      \end{align*}
    \item Avec la distribution prédictive. Nous avons
      \begin{align*}
        \Pr[&X_2 = x|X_1 = \nombre{30000}]
              = \frac{\Pr[X_2 = x, X_1 = \nombre{30000}]}{\Pr[X_1 =
              \nombre{30000}]} \\
            &= \frac{\sum_{\theta=A,B,C} \Pr[X_2 = x|\Theta = \theta]
              \Pr[X_1 = \nombre{30000}|\Theta = \theta]\Pr[\Theta =
              \theta]}%
              {\sum_{\theta=A,B,C} \Pr[X_1 = \nombre{30000}|\Theta =
              \theta]\Pr[\Theta = \theta]} \\
            &=
              \begin{cases}
                \frac{3}{4},
                & x = 0 \\
                \dfrac{(\frac{3}{4}) (\frac{1}{20}) +
                  (\frac{3}{4}) (\frac{1}{8}) +
                  (\frac{3}{4}) (\frac{3}{20})}{%
                  \frac{1}{20} + \frac{1}{8} + \frac{3}{20}},
                & x = \nombre{10000} \\
                \dfrac{(\frac{3}{20}) (\frac{1}{20}) +
                  (0) (\frac{1}{8}) +
                  (\frac{1}{20}) (\frac{3}{20})}{%
                  \frac{1}{20} + \frac{1}{8} + \frac{3}{20}},
                & x = \nombre{20000} \\
                \dfrac{(\frac{1}{20})^2 +
                  (\frac{1}{8})^2 +
                  (\frac{3}{20})^2}{%
                  \frac{1}{20} + \frac{1}{8} + \frac{3}{20}},
                & x = \nombre{30000}
              \end{cases} \\
            &=
              \begin{cases}
                \frac{3}{4}, & x = 0 \\
                \frac{6}{130}, & x = \nombre{10000} \\
                \frac{41}{520}, & x = \nombre{20000} \\
                \frac{65}{520}, & x = \nombre{30000},
              \end{cases}
      \end{align*}
      d'où
      \begin{align*}
        \esp{X_2|X_1 = \nombre{30000}}
        &= \nombre{10000} \left( \frac{6}{130} \right)
        + \nombre{20000} \left( \frac{41}{520} \right) \\
        &\phantom{=}
          + \nombre{30000} \left( \frac{65}{520} \right) \\
        &=\nombre{5788,46}.
      \end{align*}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Vous savez que le nombre de sinistres pour un assuré est distribué
  selon une loi de Poisson de paramètre aléatoire. Ce paramètre est
  distribué selon une loi gamma avec moyenne $2$ et variance $2$. De plus,
  tout sinistre est d'un montant de $1$~\$.
  \begin{enumerate}
  \item Trouver la prime qui devrait être exigée d'un nouvel assuré.
  \item Trouver la prime bayésienne pour la quatrième année si cet
    assuré a eu huit sinistres au cours des trois premières années.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $2$
    \item $2,5$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    La sévérité des sinistres ne joue aucun rôle dans ce problème.
    Posons $S|\Theta = \theta \sim \text{Poisson}(\theta)$ et
    $\Theta \sim \text{Gamma}(\alpha, \lambda)$. Nous savons que
    $\esp{\Theta} = \alpha/\lambda = 2$ et
    $\var{\Theta} = \alpha/\lambda^2 = 2$, d'où $\alpha = 2$ et
    $\lambda = 1$.
    \begin{enumerate}
    \item Nous chargeons la prime collective à un nouvel assuré. Or,
      $\esp{S} = \esp{\esp{S|\Theta}} = \esp{\Theta} = 2$.
    \item Nous cherchons $\esp{\mu(\Theta)|S_1 + S_2 + S_3 = 8}$. Or,
      \begin{align*}
        u(\theta|x_1, x_2, x_3)
        &\propto \prod_{t = 1}^3 f(x_t|\theta) u(\theta) \\
        &\propto \prod_{t = 1}^3 \theta^{x_t} e^{-\theta}
        \theta^{2 - 1} e^{-\theta} \\
        &= \theta^{\sum_{t = 1}^3 x_t + 2 - 1} e^{-4 \theta},
      \end{align*}
      d'où
      $\Theta|S_1 = x_1, S_2 = x_2, S_3 = x_3 \sim
      \text{Gamma}(\sum_{t = 1}^3 x_t + 2, 4)$. (Vous remarquerez que
      seul le nombre total de sinistres dans les trois premières
      années est important, pas les fréquences annuelles.) Enfin,
      $\esp{\mu(\Theta)|S_1 + S_2 + S_3 = 8} = \esp{\Theta|S_1 + S_2 +
        S_3 = 8} = 10/4 = 2,5$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Le nombre annuel d'accidents d'un assuré suit une loi binomiale de
  paramètre $n = 2$. Il y a toutefois incertitude quant à la
  probabilité $\theta$ que cet assuré ait un accident. Trois valeurs
  sont jugées possibles: $\frac{1}{4}$, $\frac{1}{2}$ et $\frac{1}{8}$
  et ce, avec probabilité $25$~\%, $25$~\% et $50$~\%, respectivement.
  L'assuré n'a eu aucun accident la première année et deux accidents
  la deuxième année.
  \begin{enumerate}
  \item Calculer la distribution révisée de $\Theta$ à la lumière des
    deux premières années d'expérience.
  \item Calculer le nombre espéré d'accidents de cet assuré pour la
    troisième année.
  \end{enumerate}
  \begin{rep}
    $\Pr[\Theta = 1/4|N_1 = 0, N_2 = 2] = 0,2891$,
    $\Pr[\Theta = 1/2|N_1 = 0, N_2 = 2] = 0,5141$,
    $\Pr[\Theta = 1/8|N_1 = 0, N_2 = 2] = 0,1968$.
  \end{rep}
  \begin{sol}
    Nous avons $N|\Theta = \theta \sim \text{Binomiale}(2, \theta)$ et
    \begin{equation*}
      \Pr[\Theta = \theta] =
      \begin{cases}
        0,25, & \theta = 1/4 \\
        0,25, & \theta = 1/2 \\
        0,50, & \theta = 1/8.
      \end{cases}
    \end{equation*}
    \begin{enumerate}
    \item La fonction de masse de probabilités est:
      \begin{align*}
        \Pr[\Theta = \theta|N_1 = 0&, N_2 = 2] \\
        &= \frac{\Pr[N_1 = 0, N_2 = 2|\Theta = \theta]
          \Pr[\Theta = \theta]}{%
          \sum_{\theta} \Pr[N_1 = 0, N_2 = 2|\Theta = \theta]
          \Pr[\Theta = \theta]} \\
        &= \frac{\theta^2 (1 - \theta)^2 \Pr[\Theta = \theta]}{%
          \sum_{\theta} \theta^2 (1 - \theta)^2 \Pr[\Theta = \theta]} \\
        &=
        \begin{cases}
          \frac{0,008789}{0,030396}, & \theta = 1/4 \\[6pt]
          \frac{0,015625}{0,030396}, & \theta = 1/2 \\[6pt]
          \frac{0,005981}{0,030396}, & \theta = 1/8
        \end{cases} \\
        &=
        \begin{cases}
          0,2891, & \theta = 1/4 \\
          0,5141, & \theta = 1/2 \\
          0,1968, & \theta = 1/8.
        \end{cases}
      \end{align*}
    \item Nous devons calculer $\esp{N_3|N_1 = 0, N_2 = 2}$, ce qui
      peut se faire de deux façons. Tout d'abord, en trouvant d'abord
      la distribution prédictive:
      \begin{align*}
        \Pr[N_3 = n&|N_1 = 0, N_2 = 2] \\
          &= \sum_\theta \Pr[N_3 = n|\Theta = \theta] \Pr[\Theta =
          \theta|N_1 = 0, N_2 = 2] \\
          &=
          \begin{cases}
            0,4418, & n = 0 \\
            0,4085, & n = 1 \\
            0,1497, & n = 2.
          \end{cases}
      \end{align*}
      Ainsi,
      \begin{align*}
        \esp{N_3|N_1 = 0, N_2 = 2}
        &= 0,4085 + 2 (0,1497) \\
        &= 0,7078.
      \end{align*}
      L'autre méthode, plus rapide, consiste à calculer la prime
      bayésienne à partir de la prime de risque:
      \begin{align*}
        \esp{N_3|N_1 = 0, N_2 = 2}
        &= \esp{\mu(\Theta)|N_1 = 0, N_2 = 2} \\
        &= \esp{2 \Theta|N_1 = 0, N_2 = 2} \\
        &= 2 \esp{\Theta|N_1 = 0, N_2 = 2} \\
        &= 2
        \left[
          \frac{1}{4} (0,2891) + \frac{1}{2} (0,5141) +
          \frac{1}{8} (0,1968)
        \right] \\
        &= 0,7078.
      \end{align*}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  On simule une expérience de sinistre comme suit: on lance un dé, et
  on pose le résultat égal à $\theta$. Connaissant $\theta$, on
  simule un nombre aléatoire d'une distribution uniforme sur $[0,\,
  100\theta]$.
  \begin{enumerate}
  \item Trouver la prime bayésienne si des résultats de $80$ et $340$
    ont été obtenus lors des deux premiers essais.
  \item Est-il possible d'écrire la prime bayésienne sous forme d'une
    prime de crédibilité? Si oui, trouver le facteur de crédibilité.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $236,67$
    \item non
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    Soit $S$ le résultat du tir aléatoire et $\Theta$ le résultat du
    dé. Nous avons $S|\Theta = \theta \sim U(0, 100\theta)$ et
    $\Pr[\Theta = \theta] = 1/6$.
    \begin{enumerate}
    \item Déterminons d'abord la densité à postériori de $\Theta$:
      \begin{align*}
        \Pr[\Theta = \theta|S_1 = 80, S_2 = 340)
        &= \frac{
          f(80|\theta) f(340|\theta) \Pr[\Theta = \theta]}{%
          \sum_{\theta = 1}^6 f(80|\theta) f(340|\theta)
          \Pr[\Theta = \theta]} \\
        &=
        \begin{cases}
          \frac{(\frac{1}{100})(0)}{%
            (\frac{1}{400})^2 + (\frac{1}{500})^2 + (\frac{1}{600})^2},
          & \theta = 1 \\
          \frac{(\frac{1}{200})(0)}{%
            (\frac{1}{400})^2 + (\frac{1}{500})^2 + (\frac{1}{600})^2},
          & \theta = 2 \\
          \frac{(\frac{1}{300})(0)}{%
            (\frac{1}{400})^2 + (\frac{1}{500})^2 + (\frac{1}{600})^2},
          & \theta = 3 \\
          \frac{(\frac{1}{400})(\frac{1}{400})}{%
            (\frac{1}{400})^2 + (\frac{1}{500})^2 + (\frac{1}{600})^2},
          & \theta = 4 \\
          \frac{(\frac{1}{500})(\frac{1}{500})}{%
            (\frac{1}{400})^2 + (\frac{1}{500})^2 + (\frac{1}{600})^2},
          & \theta = 5 \\
          \frac{(\frac{1}{600})(\frac{1}{600})}{%
            (\frac{1}{400})^2 + (\frac{1}{500})^2 + (\frac{1}{600})^2},
          & \theta = 6 \\
        \end{cases} \\
        &=
        \begin{cases}
          0, & \theta = 1 \\
          0, & \theta = 2 \\
          0, & \theta = 3 \\
          0,4798, & \theta = 4 \\
          0,3070, & \theta = 5 \\
          0,2132, & \theta = 6.
        \end{cases}
      \end{align*}
      Ainsi, puisque
      $\mu(\theta) = \esp{S|\Theta = \theta} = 50 \theta$, la prime
      bayésienne est
      \begin{align*}
        \esp{\mu(\Theta)|S_1 = 80, S_2 = 340}
        &=50 [(4)(0,4798) + (5)(0,3070) \\
        &\phantom{=} + (6)(0,2132)] \\
        &= 236,67.
      \end{align*}
    \item Il faudrait que
      \begin{align*}
        236,65
        &= z \bar{S} + (1 - z) \esp{\mu(\Theta)} \\
        &= z 210 + (1 - z)(50)(1 + 2 + 3 + 4 + 5 + 6)
        \left(
          \frac{1}{6}
        \right) \\
        &= 175 - 35 z,
      \end{align*}
      soit $z = 1,76$. Comme $z$ ne peut être plus grand que $1$ par
      définition, il est impossible d'écrire la prime bayésienne sous
      la forme d'une prime de crédibilité.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Une actuaire suppose que $S_t|\Theta = \theta$ suit une loi
  Gamma$(2, \theta)$ et elle fait l'hypothèse que la fonction de
  densité de probabilité de $\Theta$ est la suivante:
  \begin{equation*}
    u(\theta) =
    \begin{cases}
      \theta /50, & 0< \theta < 10 \\
      0, & \text{ailleurs}.
    \end{cases}
  \end{equation*}
  Elle simule ensuite deux valeurs de $S$ et obtient $x_1 = 1$ et
  $x_2 = 1$. Quelle est maintenant la distribution à postériori de
  $\Theta$ pour ce simulateur?
  \begin{rep}
    $u(\theta|S_1 = 1, S_2 = 1) = \theta^{5} e^{-2\theta}/1,8737$,
    $0 < \theta < 10$
  \end{rep}
  \begin{sol}
    Par le théorème de Bayes,
    \begin{align*}
      u(\theta|S_1 = 1, S_2 = 1)
      &= \frac{\prod_{t = 1}^2 f(x_t|\Theta = \theta)u(\theta)}{%
        \int_0^{10} \prod_{t = 1}^2 f(x_t|\Theta = \theta)u(\theta)\, d\theta} \\
      &= \frac{\frac{\theta}{50} \prod_{t = 1}^2 \theta^2 x_t e^{-\theta x_t}}{%
        \int_0^{10} \frac{\theta}{50}
        \prod_{t = 1}^2 \theta^2 x_t e^{-\theta x_t}\, d\theta} \\
      &= \frac{\theta^5 e^{-2\theta}}{\int_0^{10} \theta^5 e^{-2\theta}\, d\theta},
    \end{align*}
    pour $0 < \theta < 10$. Or,
    \begin{align*}
      \int_0^{10} \theta^5 e^{-2\theta}\, d\theta
      &= \frac{\Gamma(6)}{2^6}
      \int_0^{10} \frac{2^6}{\Gamma(6)} \theta^{6-1} e^{-2\theta}\, d\theta \\
      &= 1,875 G(10; 6, 2),
    \end{align*}
    où $G(x; \alpha, \lambda)$ est la fonction de répartition d'une
    loi Gamma$(\alpha, \lambda)$. On obtient avec R la valeur de
    $G(10; 6, 2)$:
<<echo=TRUE>>=
pgamma(10, 6, 2)
@
    Par conséquent, la distribution à postériori de $\Theta$ est
    \begin{equation*}
      u(\theta|S_1 = 1, S_2 = 1) =
      \begin{cases}
        \dfrac{\theta^5 e^{-2\theta}}{1,8737}, & 0 < \theta < 10 \\
        0, \text{ailleurs}.
      \end{cases}
    \end{equation*}
  \end{sol}
\end{exercice}

\begin{exercice}
  On vous donne les informations ci-dessous au sujet d'un régime
  d'assurance dentaire.
  \begin{enumerate}[i)]
  \item La fréquence des sinistres des assurés suit une loi de
    Poisson.
  \item La moitié des assurés a en moyenne deux sinistres par année.
  \item L'autre moitié a en moyenne quatre sinistres par année.
  \end{enumerate}
  Un assuré choisi au hasard au sein du portefeuille a eu quatre
  sinistres dans chacune des deux premières années.
  \begin{enumerate}
  \item Énoncer le modèle complet utilisé ici par l'assureur.
  \item Déterminer l'estimateur bayésien du nombre de sinistres de cet
    assuré pour la troisième année.
  \end{enumerate}
  \begin{rep}
    \begin{enumerate}
      \stepcounter{enumi}
    \item $3,6484$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Le modèle est le suivant: $N|\Theta \sim
      \text{Poisson}(\Theta)$ et
      \begin{equation*}
        \Pr[\Theta = \theta] =
        \begin{cases}
          0,5, & \theta = 2 \\
          0,5, & \theta = 4.
        \end{cases}
      \end{equation*}
    \item Vous devez d'abord trouver la distribution à postériori de
      $\Theta$ en utilisant
      \begin{align*}
        \Pr[N_1=4, N_2=4|\Theta = 2]
        &= \left( \frac{2^4 e^{-2}}{4!} \right)^2 = 0,008140 \\
        \Pr[N_1=4, N_2=4|\Theta = 4] &= \left( \frac{4^4
            e^{-4}}{4!} \right)^2 = 0,038168,
      \end{align*}
      d'où $\Pr[N_1=4, N_2=4] = 0,008140 (0,5) + 0,038168 (0,5) =
      0,023154$. Alors
      \begin{equation*}
        \Pr[\Theta=\theta|N_1=4, N_2=4] =
        \begin{cases}
          \frac{0,008140(0,5)}{0,023154} = 0,175779, & \theta=2 \\[6pt]
          \frac{0,038168(0,5)}{0,023154} = 0,824221, & \theta=4.
        \end{cases}
      \end{equation*}
      Ainsi, puisque $\mu(\Theta) = \Theta$, la prime bayésienne
      pour la troisième année est $\esp{\Theta|N_1=4, N_2=4} = 2
      (0,175779) + 4 (0,824221) = 3,6484$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Un portefeuille d'assurance est composé de $25$~\% de bons risques,
  $60$~\% de risques moyens et $15$~\% de mauvais risques. Tous les
  risques ont une distribution de sinistres de type gamma, mais dont
  les paramètres diffèrent selon le tableau ci-dessous.
  \begin{center}
    \begin{tabular}{lrr}
      \toprule
      Type de risque & $\alpha$ & $\lambda$ \\
      \midrule
      Bon     & $4 $ & $2$ \\
      Moyen   & $4 $ & $1$ \\
      Mauvais & $10$ & $2$ \\
      \bottomrule
    \end{tabular}
  \end{center}
  Le dossier de sinistre d'un risque choisi au hasard est de $1$ et
  $2$ au cours des deux premières années. Calculer la prime bayésienne
  de ce risque pour la troisième année.
  \begin{rep}
    $2,3107$
  \end{rep}
  \begin{sol}
    Soit $\theta_1$ représentant les bons risques, $\theta_2$ les
    moyens risques et $\theta_3$ les mauvais risques. Nous avons donc
    le modèle suivant:
    \begin{align*}
      S|\Theta = \theta_1
      &\sim \text{Gamma}(4, 2) \Leftrightarrow
      f(x|\theta_1) = \frac{8}{3} x^3 e^{-2x} \\
      S|\Theta = \theta_2
      &\sim \text{Gamma}(4, 1) \Leftrightarrow
      f(x|\theta_2) = \frac{1}{6} x^3 e^{-x} \\
      S|\Theta = \theta_3
      &\sim \text{Gamma}(10, 2) \Leftrightarrow
      f(x|\theta_3) = \frac{8}{\nombre{2835}} x^9 e^{-2x}
    \end{align*}
    et
    \begin{equation*}
      \Pr[\Theta = \theta_i] =
      \begin{cases}
        0,25, & i = 1 \\
        0,60, & i = 2 \\
        0,15, & i = 3.
      \end{cases}
    \end{equation*}
    De plus $\mu(\theta_1) = 2$, $\mu(\theta_2) = 4$ et $\mu(\theta_3)
    = 5$.  Le calcul de la prime bayésienne requiert la distribution à
    postériori de $\Theta$:
    \begin{align*}
      \Pr[\Theta = \theta_i|S_1 = 1, S_2 = 2]
      &= \frac{f(1|\theta_i) f(2|\theta_i) \Pr[\Theta = \theta_i]}{%
        \sum_{j=1}^3 f(1|\theta_j) f(2|\theta_j)
        \Pr[\Theta = \theta_j]} \\
      &=
      \begin{cases}
        0,841507, & i = 1 \\
        0,158457, & i = 2 \\
        0,000036, & i = 3.
      \end{cases}
    \end{align*}
    La prime bayésienne pour la troisième année est donc
    $\esp{\mu(\Theta)|S_1 = 1, S_2 = 2} = 2 (0,841507) + 4(0,158457) +
    5(0,000036) = 2,3170$.
  \end{sol}
\end{exercice}

\begin{exercice}
  On vous fournit l'information suivante au sujet de deux groupes de
  contrats.
  \begin{enumerate}[i)]
  \item La fréquence des sinistres des contrats du groupe A a une
    distribution de Poisson de moyenne $1$ par année.
  \item La fréquence des sinistres des contrats du groupe B a une
    distribution de Poisson de moyenne $3$ par année.
  \item Les montants de sinistres des contrats du groupe A a une
    distribution exponentielle de moyenne $1$.
  \item Les montants de sinistres des contrats du groupe B a une
    distribution exponentielle de moyenne $3$.
  \item Les deux groupes sont composés d'un nombre égal de contrats.
  \item À l'intérieur de chaque groupe, la fréquence et la sévérité
    des sinistres sont indépendantes.
  \end{enumerate}
  Un contrat choisi au hasard a deux accidents au cours de la première
  année. Le montant de ces sinistres est de $1$ et $3$.
  \begin{enumerate}
  \item Énoncer le modèle pour la fréquence et la sévérité des
    sinistres dans ce portefeuille.
  \item Calculer l'espérance à postériori du montant total des
    sinistres du contrat choisi ci-dessus. (\emph{Note}: calculer
    l'espérance du montant total des sinistres comme le produit de
    l'espérance de la fréquence et de l'espérance de la sévérité des
    sinistres.)
  \end{enumerate}
  \begin{rep}
    \begin{enumerate}
      \stepcounter{enumi}
    \item $6,286$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Soit $\Theta$ représentant le niveau de risque d'un contrat,
      avec $\theta = 1$ identifiant un risque de classe A et $\theta =
      3$ identifiant un risque de classe B. Comme chaque classe
      contient le même nombre de risques, la probabilité qu'un risque
      choisi au hasard provienne d'une classe est de $50$~\%:
      \begin{equation*}
        \Pr[\Theta = \theta] =
        \begin{cases}
          \frac{1}{2}, & \theta = 1 \\
          \frac{1}{2}, & \theta = 3.
        \end{cases}
      \end{equation*}
      Si $N$ est le nombre de sinistres pendant un an pour un contrat
      et $X$ est le montant d'un sinistre pour ce même contrat, nous avons
      le modèle suivant:
      \begin{align*}
        N|\Theta
        &\sim \text{Poisson}(\Theta) \\
        X|\Theta &\sim \text{Exponentielle}(1/\Theta).
      \end{align*}
      En effet, il est précisé dans l'énoncé que l'espérance de la loi
      exponentielle dans chaque groupe est égale à celle de la loi de
      Poisson.
    \item Pour simplifier la notation, $A$ représente l'événement
      $\{N_1=2, X_1=1, X_2=3\}$. Nous voulons calculer
      \begin{equation*}
        \esp{S|A} =
        \sum_{\theta=1,3} \esp{S|\Theta = \theta}
        \Pr[\Theta = \theta|A].
      \end{equation*}
      Premièrement, comme la fréquence et la sévérité sont
      indépendantes sachant $\Theta$ à l'intérieur de chaque classe,
      nous avons
      \begin{equation*}
        \esp{S|\Theta} = \esp{N|\Theta} \esp{X|\Theta} = \Theta^2.
      \end{equation*}
      Puis, en utilisant la règle de Bayes,
      \begin{align*}
        \Pr[\Theta=\theta|A]
        &= \frac{\Pr[A|\Theta=\theta] \Pr[\Theta=\theta]}{%
          \sum_{\theta=1,3} \Pr[A|\Theta=\theta] \Pr[\Theta=\theta]} \\
        &= \frac{(\theta^2 e^{-\theta})
          (\frac{1}{\theta} e^{-1/\theta})
          (\frac{1}{\theta} e^{-3/\theta})
          (\frac{1}{2})}{%
          \sum_{\theta=1,3} (\theta^2 e^{-\theta})
          (\frac{1}{\theta} e^{-1/\theta})
          (\frac{1}{\theta} e^{-3/\theta})
          (\frac{1}{2})} \\
        &= \frac{e^{-(\theta^2+4)/\theta}}
        {\sum_{\theta=1,3} e^{-(\theta^2+4)/\theta}} \\
        &=
        \begin{cases}
          0,3392, & \theta = 1 \\
          0,6608, & \theta = 3,
        \end{cases}
      \end{align*}
      d'où
      \begin{equation*}
        \esp{S|A} = (1)^2 (0,3392) + (3)^2 (0,6608) = 6,286.
      \end{equation*}
    \end{enumerate}
  \end{sol}
\end{exercice}


%%%
%%% Crédibilité bayésienne exacte
%%%

\begin{exercice}
  \label{exercice:bayesienne:geometrique-beta}
  Soit un modèle géométrique/bêta, c'est-à-dire
  \begin{gather*}
    \Pr[S = x|\Theta = \theta]
    = \theta (1 - \theta)^x,   \quad x = 0, 1, \dots \\
    \intertext{et}
    u(\theta)
    = \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)}\,
       \theta^{a-1} (1 - \theta)^{b-1}, \quad 0 < \theta < 1.
  \end{gather*}
  \begin{enumerate}
  \item Calculer la prime de risque.
  \item Calculer la prime collective.
  \item Calculer la distribution à postériori de $\Theta$ après $n$
    années d'expérience $S_1, \dots, S_n$.
  \item Calculer la distribution prédictive de $S_{n+1}$.
  \item Calculer la prime bayésienne à partir du résultat en c) ou
    celui en d). Pourquoi avoir choisi une approche plutôt qu'une
    autre?
  \item Exprimer la prime bayésienne en e) comme une prime de
    crédibilité.
  \end{enumerate}
  \begin{rep}
    Voir le tableau de l'\autoref{chap:formules}.
  \end{rep}
  \begin{sol}
    Nous avons $S|\Theta \sim \text{Géométrique}(\Theta)$ et
    $\Theta \sim \text{Bêta}(a, b)$.
    \begin{enumerate}
    \item $\mu(\Theta) = \esp{S|\Theta} = \sum_{x=0}^\infty x \theta
      (1 - \theta)^x = (1 - \theta)/\theta$. (Ce résultat est aisément
      obtenu en dérivant $\sum_{x=0}^\infty \theta (1 - \theta)^x =
      1$.)
    \item La prime collective est
      \begin{align*}
        m = \esp{\mu(\Theta)}
        &= \frac{\Gamma(a +
          b)}{\Gamma(a) \Gamma(b)}
        \int_0^1 \theta^{a - 2} (1 - \theta)^b\, d\theta \\
        &= \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)}
        \frac{\Gamma(a - 1) \Gamma(b + 1)}{\Gamma(a +
          b)} \\
        &= \frac{b}{a - 1}.
      \end{align*}
    \item Soit $\mat{S} = (S_1, \dots, S_n)$ et $\mat{x} = (x_1,
      \dots, x_n)$. Premièrement, $\Pr[\mat{S} = \mat{x}|\Theta =
      \theta] = \prod_{t=1}^n \Pr[S_t = x_t|\Theta = \theta] =
      \theta^n (1 - \theta)^{\sum x_t}$.  Ensuite,
      \begin{align*}
        u(\theta|\mat{x})
        &\propto \theta^n (1 - \theta)^{\sum x_t}
        \theta^{a - 1} (1 - \theta)^{b - 1} \\
        &= \theta^{a + n - 1}(1 - \theta)^{b + \sum x_t - 1},
      \end{align*}
      d'où $\Theta|\mat{S} = \mat{x} \sim \text{Bêta}(\tilde{a} =
      a + n, \tilde{b} = b + \sum x_t)$.
    \item Comme la densité à postériori de $\Theta$ est du même type
      que sa densité à priori, nous pouvons calculer en premier la
      densité marginale de $S$. La distribution prédictive sera du
      même type, mais avec des paramètres modifiés. Or,
      \begin{align*}
        \Pr[S = x]
        &= \int_0^1 \Pr[S = x|\Theta = \theta] u(\theta)\, d\theta \\
        &= \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)}
        \int_0^1 \theta^a (1 - \theta)^{b + x - 1}\, d\theta
        \\
        &= \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)}
        \frac{\Gamma(a + 1) \Gamma(b + x)}{\Gamma(a +
          b +x + 1)}
      \end{align*}
      et donc
      \begin{equation*}
        \Pr[S_{n+1}=x|\mat{S}=\mat{x}] =
        \frac{\Gamma(\tilde{a} + \tilde{b})}
        {\Gamma(\tilde{a}) \Gamma(\tilde{b})}
        \frac{\Gamma(\tilde{a} + 1) \Gamma(\tilde{b} + x)}
        {\Gamma(\tilde{a} + \tilde{b} +x + 1)},
      \end{equation*}
      avec $\tilde{a} = a + n$ et $\tilde{b} = b +
      \sum x_t$.
    \item Utilisons le résultat en c) car trouver l'espérance de la
      distribution prédictive trouvée en d) peut être quelque peu
      compliqué. Des résultats de b) et c),
      \begin{equation*}
        \esp{\mu(\Theta)|\mat{S}} =
        \frac{\tilde{b}}{\tilde{a} - 1} =
        \frac{b + \sum_{t = 1}^n S_t}{a + n - 1}.
      \end{equation*}
    \item $\esp{\mu(\Theta)|\mat{S}} = z \bar{S} + (1 - z) m$ avec $z
      = n/(n + a - 1)$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{exercice:bayesienne:binomiale-beta}
  Soit $S|\Theta \sim \text{Binomiale}(\nu, \Theta)$ et $\Theta \sim
  \text{Bêta}(a, b)$.
  \begin{enumerate}
  \item Déterminer la prime de risque.
  \item Déterminer la prime collective.
  \item Déterminer la distribution à postériori de $\Theta$ après $n$
    années.
  \item Déterminer la prime bayésienne pour la ($n+1$){\ieme} année et
    vérifier si celle-ci peut s'exprimer comme une prime de
    crédibilité ou non.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $\nu \Theta$
    \item $\nu a/(a + b)$
    \item Bêta$(a + \sum_{t=1}^n S_t, b + n \nu -
      \sum_{t=1}^n S_t)$
    \item $z = n/(n + (a + b)/\nu)$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    Nous avons $S|\Theta \sim \text{Binomiale}(\nu, \Theta)$ et
    $\Theta \sim \text{Bêta}(a, b)$.
    \begin{enumerate}
    \item La prime de risque est $\mu(\Theta) = \esp{S|\Theta} = \nu
      \Theta$.
    \item La prime collective est $\esp{\mu(\Theta)} = \nu
      \esp{\Theta} = \nu a/(a + b)$.
    \item La distribution à postériori de $\Theta$ sachant
      $S_1 = x_1, \dots, S_n = x_n$ est
      \begin{align*}
        u(\theta|x_1, \dots, x_n)
        &\propto u(\theta) \prod_{t=1}^n \Pr[S = x_t|\Theta = \theta] \\
        &\propto \theta^{a-1} (1 - \theta)^{b-1}
        \prod_{t=1}^n \theta^{x_t} (1 - \theta)^{\nu - x_t} \\
        &= \theta^{a + \sum x_t - 1} (1 - \theta)^{b + n \nu
          + \sum x_t - 1}.
      \end{align*}
      Il s'agit d'une distribution bêta avec paramètres $\tilde{a} =
      a + \sum_{t=1}^n x_t$ et $\tilde{b} = b + n \nu -
      \sum_{t=1}^n x_t$.
    \item Comme la distribution à postériori est de même famille que
      la distribution à priori, la prime bayésienne est de la même
      forme que la prime collective, mais avec des paramètres
      modifiés:
      \begin{align*}
        \esp{\mu(\Theta)|S_1, \dots, S_n}
        &= \frac{\nu \tilde{a}}{\tilde{a} + \tilde{b}} \\
        &= \frac{\nu (a + \sum_{t=1}^n S_t)}{n \nu + a + b} \\
        &= \left(\frac{n\nu}{n\nu + a + b}\right)
        \left(\frac{\sum_{t=1}^n S_t}{n}\right) \\
        &\phantom{=}
        + \left(\frac{a + b}{n\nu + a + b}\right)
        \left(\frac{\nu a}{a + b}\right) \\
        &= z \bar{S} + (1 - z) \esp{\mu(\Theta)}
      \end{align*}
      où
      \begin{equation*}
        z = \frac{n}{n + (a + b)/\nu}.
      \end{equation*}
      Cette combinaison de distributions est la convolution du cas
      Bernoulli/Bêta, la binomiale étant une somme de Bernoulli.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{exercice:bayesienne:gamma-gamma}
  Soit $S|\Theta \sim \text{Gamma}(\tau, \Theta)$ et $\Theta \sim
  \text{Gamma}(\alpha, \lambda)$.
  \begin{enumerate}
  \item Déterminer la distribution marginale de $S$. Identifier cette
    distribution à trois paramètres.
  \item Calculer la prime de risque.
  \item Calculer la prime collective, d'abord à l'aide de la
    distribution marginale de $S$, puis comme la moyenne des primes de
    risque.
  \item Déterminer la distribution à postériori de $\Theta$ après $n$
    années d'expérience $S_1, \dots, S_n$.
  \item Déterminer la distribution prédictive de $S_{n+1}$.
  \item Calculer la prime bayésienne, d'abord à partir de la
    distribution à postériori de $\Theta$, puis à partir de la
    distribution prédictive.
  \item La prime bayésienne est-elle une prime de crédibilité?
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item Pareto généralisée
    \item $\tau/\Theta$
    \item $\tau \lambda/(\alpha - 1)$
    \item Gamma$(\alpha + n\tau, \lambda + \sum_{t=1}^n S_t)$
    \item Pareto généralisée$(\alpha + n\tau, \lambda + \sum_{t=1}^n
      S_t, \tau)$
    \item $\tau (\lambda + \sum_{t=1}^n S_t)/(\alpha + n\tau - 1)$
    \item $z = n/(n + (\alpha - 1)\tau^{-1})$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    Nous avons $S|\Theta \sim \text{Gamma}(\tau, \Theta)$ et $\Theta
    \sim \text{Gamma}(\alpha, \lambda)$.
    \begin{enumerate}
    \item La densité marginale de $S$ est calculée comme suit:
      \begin{align*}
        f(x)
        &= \int_0^\infty f(x|\theta)\, u(\theta)\, d\theta \\
        &= \frac{\lambda^\alpha x^{\tau-1}}{\Gamma(\tau)
          \Gamma(\alpha)} \int_0^\infty \theta^\tau e^{-\theta x}
        \theta^{\alpha-1}
        e^{-\lambda \theta}\, d\theta \\
        &= \frac{\lambda^\alpha x^{\tau-1}}{\Gamma(\tau)
          \Gamma(\alpha)} \frac{\Gamma(\alpha +
          \tau)}{(x+\lambda)^{\alpha + \tau}},
      \end{align*}
      laquelle est une densité de Pareto généralisée avec paramètres
      $\alpha$, $\lambda$ et $\tau$.
    \item La prime de risque est $\mu(\Theta) = \esp{S|\Theta} =
      \tau/\Theta$.
    \item De la distribution marginale de $S$, nous avons
      $m = \esp{S} = \tau \lambda/(\alpha - 1)$
      (\autoref{sec:distributions:paretogen}). De la prime de risque,
      $m = \tau \esp{1/\Theta} = \tau \lambda/(\alpha - 1)$. Les deux
      approches sont équivalentes.
    \item La densité de la distribution à postériori de $\Theta$ est
      \begin{align*}
        u(\theta|x_1, \dots, x_n)
        &\propto u(\theta) \prod_{t=1}^n f(x_t|\theta) \\
        &\propto \theta^{\alpha-1} e^{-\lambda \theta}
        \prod_{t=1}^n \theta^\tau e^{-\theta x_t} \\
        &= \theta^{\alpha + n\tau-1} e^{-\left(\lambda + \sum x_t\right) \theta},
      \end{align*}
      d'où la distribution à postériori de $\Theta$ est une gamma avec
      paramètres $\tilde{\alpha} = \alpha + n\tau$ et $\tilde{\lambda}
      = \lambda + \sum_{t=1}^n x_t$. La distribution à priori de
      $\Theta$ est la conjuguée naturelle de la fonction de
      vraisemblance $f(x|\theta)$.
    \item La distribution prédictive est une Pareto généralisée avec
      paramètres $\tilde{\alpha} = \alpha + n\tau$, $\tilde{\lambda} =
      \lambda + \sum_{t=1}^n x_t$ et $\tau$.
    \item En utilisant la distribution à postériori ou la distribution
      prédictive, la prime bayésienne pour l'année $n + 1$ est
      simplement la prime collective avec les paramètres modifiés.
      Par conséquent,
      \begin{align*}
        \esp{\mu(\Theta)|S_1, \dots, S_n}
        &= \tau \left(\frac{\tilde{\lambda}}{\tilde{\alpha} - 1}\right) \\
        &= \tau
        \left(
          \frac{\lambda + \sum_{t=1}^n S_t}{\alpha + n\tau - 1}
        \right).
      \end{align*}
    \item La prime bayésienne ci-dessus peut être réécrite comme suit:
      \begin{align*}
        \esp{\mu(\Theta)|S_1, \dots, S_n}
        &= \left(\frac{n\tau}{n\tau + \alpha - 1}\right) \sum_{t=1}^n
        \frac{S_t}{n} + \left(\frac{\alpha - 1}{n\tau + \alpha - 1}\right)
        \left(\frac{\tau \lambda}{\alpha-1}\right) \\
        &= z \bar{S} + (1 - z) m
      \end{align*}
      avec
      \begin{equation*}
        z = \frac{n}{n + (\alpha - 1)/\tau}.
      \end{equation*}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $S|\Theta = \theta \sim \text{Exponentielle}(\theta)$,
  $\Theta \sim \text{Gamma}(7, 42)$, la prime bayésienne de
  cinquième année est $9$ et celle de sixième année, $8,5$. Trouver
  $x_5$.
  \begin{rep}
    $3,5$
  \end{rep}
  \begin{sol}
    Dans le cas exponentielle/gamma, la prime bayésienne est
    \begin{equation*}
      B_{n + 1} = \frac{\lambda + \sum_{t=1}^n x_t}{\alpha + n - 1}.
    \end{equation*}
    Ici $\alpha = 7$ et $\lambda = 42$. Si $B_5 = 9$, alors $42 +
    \sum_{t=1}^4 x_t = 9(7 + 4 - 1) = 90$. Par conséquent,
    \begin{align*}
      B_6
      &= \frac{42 + \sum_{t=1}^4 x_t + x_5}{7 + 5 - 1} \\
      &= \frac{90 + x_5}{11} \\
      &= 8,5,
    \end{align*}
    d'où $x_5 = 3,5$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Vous utilisez un modèle Poisson/gamma pour la tarification
  bayésienne d'un contrat d'assurance. Les paramètres du modèle sont
  tels qu'après quatre années le facteur de crédibilité de ce contrat
  serait de $0,8$. Si vous changez les hypothèses de telle sorte que
  la variance de la distribution de $\Theta$ est doublée, mais que
  l'espérance demeure inchangée, combien d'années faudra-t-il au
  contrat pour atteindre un niveau de crédibilité de $0,8$?
  \begin{rep}
    $2$
  \end{rep}
  \begin{sol}
    Dans le modèle Poisson/gamma, $z = n/(n + \lambda)$. Si $z = 0,8$
    quand $n = 4$, alors $\lambda = 1$. Pour doubler la variance d'une
    distribution gamma sans changer son espérance, il faut diminuer de
    moitié chacun des paramètres. Le nouveau paramètre $\lambda$ est
    donc $1/2$. Nous cherchons la nouvelle valeur de $n$ telle que
    $n/(n + 1/2) = 0,8$, d'où $n = 2$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Pour un modèle Poisson/gamma, on vous donne
  \begin{equation*}
    \Pr[S_3 = x | S_1 = 1, S_2 = 2] = \binom{6 + x}{x} (0,9)^7
    (0,1)^x, \quad x = 0, 1, \dots.
  \end{equation*}
  Quelle est l'espérance de la distribution à priori de $\Theta$?
  \begin{rep}
    $4/7$
  \end{rep}
  \begin{sol}
    Dans le modèle Poisson/gamma, nous savons que la distribution
    prédictive est une binomiale négative avec paramètres
    $r = \alpha + \sum_{t=1}^n x_t$ et
    $\theta = (\lambda + n)/(\lambda + n + 1)$. Ici, nous pouvons
    identifier $r = 7$, $\theta = 0,9$ et nous avons $n = 2$ et
    $x_1 + x_2 = 3$. Par conséquent, $\alpha = 4$, $\lambda = 7$ et
    l'espérance de la distribution à priori --- une gamma --- est
    $4/7$.
  \end{sol}
\end{exercice}

\begin{exercice}
  La distribution de $S_t|\Theta = \theta$ est une exponentielle de
  paramètre $(\theta)$ et la distribution à priori de $\Theta$ est une
  gamma de paramètre de forme $2$ et de moyenne $1/4$. Calculer
  $\Pr[S_4 \leq 5|S_1 = 2, S_2 = 3, S_3 = 7]$.
  \begin{rep}
    $0,67$
  \end{rep}
  \begin{sol}
    Nous avons un modèle exponentielle/gamma. Comme le confirme le tableau de
    l'\autoref{chap:formules}, la distribution marginale de $S$ dans
    un tel cas est une Pareto$(\alpha, \lambda)$ et la distribution de
    $\Theta|S_1 = x_1, \dots, S_n = x_n$ une
    Gamma$(\alpha + n, \lambda + \sum_{t = 1}^n x_t)$. Par conséquent,
    la distribution de $S|S_1 = x_1, \dots, S_n = x_n$ est une
    Pareto$(\alpha + n, \lambda + \sum_{t = 1}^n x_t)$ et
    \begin{align*}
      \Pr(S_4 \leq 5|S_1 = 2, S_2 = 3, S_3 = 7)
      &= 1 - \left( \frac{8 + 12}{8 + 12 + 5} \right)^{2 + 3} \\
      &= 0,6723.
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  Un contrat d'assurance a encouru les sinistres suivants sur une
  période de cinq années: $3$, $1$, $5$, $4$, $2$. Vous utilisez un
  modèle de tarification bayésienne Poisson/gamma. Calculez la prime
  de crédibilité de ce contrat pour la sixième année pour chacune des
  combinaisons de paramètres de la distribution gamma ci-dessous.
  Interprétez les différences entre les primes de crédibilité.
  \begin{enumerate}
  \item $\alpha = 10$, $\lambda = 5$
  \item $\alpha = 50$, $\lambda = 25$
  \item $\alpha = \frac{1}{2}$, $\lambda = \frac{1}{4}$
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $2,5$
    \item $2,17$
    \item $2,9524$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    Nous savons que, dans le cas Poisson/gamma, $z = n/(n + \lambda)$.
    Ici, $n = 5$, et $\bar{S} = (3 + 1 + 5 + 4 + 2)/5 = 3$. De plus,
    la prime collective, $m$, est égale à $2$ dans tous les cas
    ci-dessous. Nous effectuons les calculs de primes bayésiennes avec
    ces résultats ainsi qu'avec la fonction \code{cm} du paquetage
    \pkg{actuar}.
    \begin{enumerate}
    \item Ici, $z = 0,5$ et donc $\pi_6 = 2,5$.
<<echo=TRUE>>=
x <- c(3, 1, 5, 4, 2)
predict(cm("bayes", x, likelihood = "poisson",
           shape = 10, rate = 5))
@
    \item Ici, $z = 1/6$ et donc $\pi_6 = 2,17$.
<<echo=TRUE>>=
predict(cm("bayes", x, likelihood = "poisson",
           shape = 50, rate = 25))
@
    \item Ici, $z = 0,9524$ et donc $\pi_6 = 2,9524$.
<<echo=TRUE>>=
predict(cm("bayes", x, likelihood = "poisson",
           shape = 0.5, rate = 0.25))
@
    \end{enumerate}
    Plus le paramètre $\lambda$ de la distribution gamma est petit,
    moins certaine est la valeur du paramètre de risque $\theta$. Par
    conséquent, nous accordons plus d'importance à l'expérience
    individuelle en augmentant le facteur de crédibilité.
  \end{sol}
\end{exercice}

\begin{exercice}
  La distribution marginale du montant total des sinistres d'un
  contrat d'assurance est
  \begin{equation*}
    f(x) = \frac{\nombre{1500}}{(100 + x)^{2,5}}, \quad x > 0.
  \end{equation*}
  Sous les hypothèses usuelles en théorie de la crédibilité, quel est
  le montant total des sinistres espéré après cinq années sans
  accident?
  \begin{rep}
    $200/11$
  \end{rep}
  \begin{sol}
    La distribution marginale donnée est une Pareto$(1,5,\, 100)$. Or,
    nous savons que la Pareto est la distribution marginale dans le
    mélange exponentielle/gamma et que, dans ce cas, la distribution
    prédictive sera une Pareto$(1,5 + n, 100 + \sum_{t=1}^n S_t)$. Par
    conséquent,
    \begin{equation*}
      \esp{S_6|S_1 = S_2 = \dots = S_5 = 0}
      = \frac{100}{1,5 + 5 - 1}
      = \frac{200}{11}.
    \end{equation*}
  \end{sol}
\end{exercice}

\begin{exercice}
  Les montants de sinistres d'un contrat furent de $S_1 = 7$, $S_2 =
  13$, $S_3 = 1$, $S_4 = 4$ au cours des quatre premières années qu'il
  était couvert par votre compagnie d'assurance. Votre expérience
  antérieure avec ce type de contrat vous permet de postuler le modèle
  suivant pour les montants de sinistres de ce contrat:
  \begin{align*}
    \Pr[S = x|\Theta = \theta]
    &= \binom{x + 4}{4} \theta^5 (1 - \theta)^x, \quad x = 0, 1, \dots, \\
    u(\theta)
    &= 504\, \theta^5 (1 - \theta)^3, \quad 0 < \theta < 1.
  \end{align*}
  Calculer la prime bayésienne de cinquième année.
  \begin{rep}
    $5,8$
  \end{rep}
  \begin{sol}
    Il est utile de remarquer ici que $X|\Theta \sim \text{Binomiale
      négative}(5, \Theta)$ et $\Theta \sim \text{Bêta}(6, 4)$. Par
    conséquent, $\mu(\Theta) = r(1 - \Theta)/\Theta = 5(1 -
    \Theta)/\Theta$ et
    \begin{align*}
      u(\theta|x_1=7, x_2=13, x_3=1, x_4=4)
      &\propto \theta^5 (1 - \theta)^3
      \prod_{t=1}^4 \theta^5 (1 - \theta)^{x_t} \\
      &= \theta^{25} (1 - \theta)^{3+\sum x_t} \\
      &= \theta^{25} (1 - \theta)^{28},
    \end{align*}
    d'où $\Theta|X_1 = 7, X_2 = 13, X_3 = 1, X_4 = 4 \sim
    \text{Bêta}(26, 29)$. Ainsi,
    \begin{align*}
      \esp{\mu(\Theta)|X_1 = 7, X_2 = 13&, X_3 = 1, X_4 = 4} \\
      &= 5\, \frac{\Gamma(55)}{\Gamma(26) \Gamma(29)}
      \int_0^1 \theta^{24} (1 - \theta)^{29}\, d\theta \\
      &= 5\, \left(\frac{\Gamma(55)}{\Gamma(26) \Gamma(29)}\right)
      \left(\frac{\Gamma(25) \Gamma(30)}{\Gamma(55)}\right) \\
      &= \frac{29}{5} \\
      &= 5,8.
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  Pour un certain modèle Poisson/gamma, nous avons
  \begin{equation*}
    \Pr[S_3 = s_{3}|S_1 = 1, S_2 = 2] =
    \binom{6 + s_3}{s_3} (0,9)^7 (0,1)^{s_3}, \quad
    s_3 = 0, 1, \dots.
  \end{equation*}
  Trouver la covariance entre $S_{1}$ et $S_{2}$.
  \begin{rep}
    $4/49$
  \end{rep}
  \begin{sol}
    Nous avons le modèle suivant :
    \begin{align*}
      S|\Theta = \theta
      &\sim \text{Poisson}(\theta) \\
      \Theta
      &\sim \text{Gamma}(\alpha, \lambda)
    \end{align*}
    La covariance entre $S_1$ et $S_2$ est
    \begin{align*}
      \Cov(S_1, S_2)
      &= \esp{\Cov(S_1, S_2|\Theta)} +
      \Cov(\esp{S_1|\Theta}, \esp{S_2|\Theta}) \\
      &= \var{\mu(\Theta)} \\
      &= \var{\Theta} \\
      &= \frac{\alpha}{\lambda^2}.
    \end{align*}
    Or, nous savons que dans le modèle Poisson/gamma, la distribution
    prédictive est une binomiale négative de paramètres
    $r = \alpha + \sum x_j$ et $p = (\lambda + n)/(\lambda + n + 1)$.
    Nous avons donc $7 = \alpha + 3$ d'où $\alpha = 4$ et
    $0,9 = (\lambda + 2)/(\lambda + 3)$ d'où $\lambda = 7$. Par
    conséquent, $\Cov{(S_1, S_2)} = \frac{4}{49}$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Cet exercice sert à démontrer le résultat obtenu par
  \cite{Jewell:exact:1974}, à savoir que la prime bayésienne issue
  d'un mélange d'une distribution de la famille exponentielle avec sa
  conjuguée naturelle est une prime de crédibilité. Soit la variable
  aléatoire $S|\Theta = \theta$ dont la distribution est membre de la
  famille exponentielle univariée, c'est-à-dire que
  \begin{equation*}
    f(x|\theta) = \frac{p(x) e^{-\theta x}}{q(\theta)},
  \end{equation*}
  où $p(\cdot)$ et $q(\cdot)$ sont des fonctions quelconques.
  \begin{enumerate}
  \item Démontrer que la conjuguée naturelle de $f(x|\theta)$ est
    \begin{equation*}
      u(\theta) = \frac{q(\theta)^{-t_0} e^{-\theta x_0}}{d(t_0, x_0)},
    \end{equation*}
    où $t_0 > 0$, $x_0 > 0$ et $d(t_0, x_0) = \int_{-\infty}^\infty
    q(\theta)^{-t_0} e^{-\theta x_0}\, d\theta$.  (\emph{Astuce}:
    démontrer que la distribution à postériori $u(\theta|x_1, \dots,
    x_n)$ est de la même famille que la distribution à priori
    $u(\theta)$.)
  \item Démontrer que
    \begin{equation*}
      \mu(\theta) = - \frac{q^\prime(\theta)}{q(\theta)} =
      - \frac{d}{d\theta} \ln q(\theta).
    \end{equation*}
  \item Démontrer que
    \begin{equation*}
      \frac{d}{d\theta} u(\theta) = (t_0 \mu(\theta) - x_0) u(\theta).
    \end{equation*}
  \item En intégrant l'équation en c) de part et d'autre par rapport à
    $\theta$ et en supposant que $u(\theta) = 0$ aux deux extrémités
    de son domaine de définition, démontrer que la prime collective
    est $\esp{\mu(\Theta)} = x_0/t_0$.
  \item Avec ce qui précède, trouver la prime bayésienne et démontrer
    qu'il s'agit d'une prime de crédibilité.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item Nous devons démontrer que la densité à postériori
      $u(\theta|x_1, \dots, x_n)$ est de la même famille que la
      densité à priori $u(\theta)$. De manière usuelle,
      \begin{align*}
        u(\theta|x_1, \dots, x_n)
        &\propto u(\theta) \prod_{t=1}^n f(x_t|\theta) \\
        &\propto q(\theta)^{-t_0} e^{-\theta x_0}
        \prod_{t=1}^n \frac{e^{-\theta x_t}}{q(\theta)} \\
        &= q(\theta)^{-t_0-n} e^{-\theta (x_0 + \sum x_t)}
      \end{align*}
      qui est, en effet, de la même famille que $u(\theta)$ avec
      paramètres modifiés $\tilde{t}_0 = t_0 + n$ et $\tilde{x}_0 =
      x_0 + \sum_{t=1}^n x_t$.
    \item Premièrement, remarquons que
      $q(\theta) = \int_{-\infty}^\infty p(x) e^{-\theta x}\, dx$ pour
      faire de $f(x|\theta)$ une densité. Ensuite,
      \begin{align*}
        \mu(\theta)
        &= \int_{-\infty}^\infty x f(x|\theta)\, dx \\
        &= \frac{1}{q(\theta)}
        \int_{-\infty}^\infty x p(x) e^{-\theta x}\, dx \\
        &= \frac{1}{q(\theta)} \int_{-\infty}^\infty
        \frac{d}{d\theta} (-p(x) e^{-\theta x})\, dx \\
        &= - \frac{1}{q(\theta)} \frac{d}{d\theta}
        \int_{-\infty}^\infty p(x) e^{-\theta x}\, dx \\
        &= - \frac{q^\prime(\theta)}{q(\theta)} = - \frac{d}{d\theta} \ln
        q(\theta).
      \end{align*}
    \item Nous avons
      \begin{align*}
        \frac{d}{d\theta} u(\theta) &= \frac{1}{d(x_0, t_0)}
        \frac{d}{d\theta} q(\theta)^{-t_0} e^{-\theta x_0} \\
        &= \frac{1}{d(x_0, t_0)} \left[ -t_0 q(\theta)^{-t_0-1}
          q^\prime(\theta) e^{-\theta x_0} - x_0 q(\theta)^{-t_0} e^{-\theta
            x_0}
        \right] \\
        &= \left[ t_0 \left( - \frac{q^\prime(\theta)}{q(\theta)} \right) -
          x_0 \right]
        \frac{q(\theta)^{-t_0} e^{-\theta x_0}}{d(x_0, t_0)} \\
        &= (t_0 \mu(\theta) - x_0) u(\theta).
      \end{align*}
    \item Supposons, sans perte de généralité, que le domaine de
      définition de la densité obtenue en c) est $(-\infty, \infty)$.
      En supposant que $u(-\infty) = u(\infty) = 0$, nous obtenons
      \begin{equation*}
        \int_{-\infty}^\infty \frac{d}{d\theta} u(\theta)\, d\theta = 0.
      \end{equation*}
      Or,
      \begin{align*}
        \int_{-\infty}^\infty \frac{d}{d\theta} u(\theta)\, d\theta
        &= t_0 \int_{-\infty}^\infty \mu(\theta) u(\theta)\, d\theta -
        x_0 \int_{-\infty}^\infty u(\theta) \\
        &= t_0 \esp{\mu(\Theta)} - x_0,
      \end{align*}
      d'où $\esp{\mu(\Theta)} = x_0/t_0$.
    \item Nous savons que la prime bayésienne est de la même forme que
      la prime collective, mais avec des paramètres modifiés. Nous avons
      donc
      \begin{align*}
        \esp{\mu(\Theta)|X_1, \dots, X_n}
        &= \frac{\tilde{x}_0}{\tilde{t}_0} \\
        &= \frac{x_0 + \sum_{t=1}^n X_t}{t_0 + n} \\
        &= \left(\frac{n}{n + t_0}\right) \bar{X} + \left(\frac{t_0}{n + t_0}\right)
        \left(\frac{x_0}{t_0}\right),
      \end{align*}
      qui est une prime de crédibilité.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  La distribution de la variable aléatoire $S|\Theta = \theta$ est
  exponentielle de moyenne $\theta$ et celle de $\Theta$ est une gamma
  de paramètres $\alpha$ et $\lambda$. Démontrer que, dans un tel cas,
  la distribution gamma n'est pas la conjuguée naturelle de
  l'exponentielle.
  \begin{sol}
    Nous avons
    \begin{align*}
      f(x|\theta)
      &= \frac{1}{\theta}\, e^{-x/\theta}, \quad x > 0 \\
      \intertext{et}
      u(\theta)
      &= \frac{\lambda^\alpha}{\Gamma(\alpha)}\, \theta^{\alpha-1}
      e^{-\lambda \theta}, \quad \theta > 0.
    \end{align*}
    La distribution à postériori de $\Theta$ est
    \begin{align*}
      u(\theta|x_1, \dots, x_n)
      &\propto \theta^{\alpha-1} e^{-\lambda \theta} \prod_{t=1}^n
      \theta^{-1} e^{\theta^{-1} x_t} \\
      &= \theta^{\alpha - n - 1} e^{-\lambda \theta + \theta^{-1} \sum
        x_t},
    \end{align*}
    qui n'est clairement pas une gamma. Donc, la gamma n'est pas la
    conjuguée naturelle de l'exponentielle avec moyenne $\Theta$. (La
    gamma inverse l'est, par contre.)
  \end{sol}
\end{exercice}

\begin{exercice}
  Démontrer que $\Theta \sim \text{Gamma}(\alpha, \lambda)$ est la
  conjuguée naturelle de $S|\Theta = \theta \sim \text{Gamma}(\alpha,
  \theta)$ et trouver les paramètres de la distribution à postériori
  de $\Theta$.
  \begin{sol}
    Voir l'\autoref{exercice:bayesienne:gamma-gamma}.
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{exercice:bayesienne:pareto-gamma}
  Cet exercice est dérivé de
  \citet[section~2.6]{Buhlmann:credibilitybook:2005}. La distribution
  de Pareto translatée (ou «Pareto à un paramètre», dans la
  terminologie de \citet{Klugman:lossmodels:4e:2012} aussi utilisée
  dans \pkg{actuar}) est très utile en réassurance pour la
  modélisation des montants extrêmes de sinistres au-delà d'un certain
  seuil. La fonction de densité de probabilité de la Pareto translatée
  de paramètre $\theta$ est
  \begin{equation*}
    f(x) = \frac{\theta x_0^\theta}{x^{\theta + 1}}, \quad x > x_0.
  \end{equation*}
  et son espérance est $x_0 \theta/(\theta - 1)$. Dans la suite, nous
  nous intéresserons à l'estimation bayésienne non pas de la prime de
  risque, mais bien seulement du paramètre $\theta$.
  \begin{enumerate}
  \item Démontrer que l'estimateur du maximum de vraisemblance du
    paramètre $\theta$ de la loi de Pareto translatée à partir d'un
    échantillon aléatoire $X_1, \dots, X_n$ est
    \begin{equation*}
      \hat{\theta}^{\text{EMV}} =
      \frac{n}{\sum_{i = 1}^n \ln (X_i/x_0)}.
    \end{equation*}
  \item Démontrer que la loi gamma de paramètres $\alpha$ et $\lambda$
    est la conjuguée naturelle de la loi de Pareto translatée pour le
    paramètre $\theta$.
  \item Démontrer que l'estimateur bayésien du paramètre $\theta$ peut
    s'exprimer sous la forme
    \begin{equation*}
      \hat{\Theta} = \eta \hat{\theta}^{\text{EMV}} +
      (1 - \eta) \frac{\alpha}{\lambda}.
    \end{equation*}
    C'est un estimateur linéaire, mais s'agit-il d'une prime de
    crédibilité?
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item La fonction de log-vraisemblance est
      \begin{align*}
        \ell(\theta)
        &= \sum_{i = 1}^n \ln f(x_i; \theta) \\
        &= \sum_{i = 1}^n (\ln \theta + \theta \ln x_0 - (\theta + 1) \ln x_i) \\
        &= n \ln \theta + n \theta \ln x_0 -
          (\theta + 1) \sum_{i = 1}^n \ln x_i, \\
        \intertext{d'où}
        \ell^\prime(\theta)
        &= \frac{n}{\theta} - n \ln x_0 - \sum_{i = 1}^n \ln x_i \\
        &= \frac{n}{\theta} - \sum_{i = 1}^n (\ln x_i - \ln x_0) \\
        &= \frac{n}{\theta} - \sum_{i = 1}^n \ln \left(
          \frac{x_i}{x_0} \right).
      \end{align*}
      En résolvant pour $\theta$ l'équation $\ell^\prime(\theta) = 0$,
      nous obtenons aisément l'estimateur du maximum de vraisemblance
      \begin{equation*}
        \hat{\theta}^{\text{EMV}} =
        \frac{n}{\sum_{i = 1}^n \ln (X_i/x_0)}.
      \end{equation*}
    \item Nous devons démontrer si $S|\Theta = \theta \sim
      \text{Pareto translatée}(\theta)$ et que $\Theta \sim
      \text{Gamma}(\alpha, \lambda)$, alors la distribution de
      $\Theta|S_1 = x_1, \dots, S_n = x_n$ est aussi une gamma avec de
      nouveaux paramètres. Il est plus simple, ici, de travailler avec
      la densité de la Pareto translatée exprimée sous la forme
      \begin{equation*}
        f(x|\theta) =
        \frac{\theta}{x_0} \left( \frac{x}{x_0} \right)^{-(\theta + 1)}.
      \end{equation*}
      Ensuite, nous avons, comme d'habitude,
      \begin{align*}
        u(\theta|x_1, \dots, x_n)
        &\propto u(\theta) \prod_{i = 1}^n f(x_i|\theta) \\
        &\propto \theta^{\alpha - 1} e^{-\lambda \theta}
          \prod_{i = 1}^n \theta
          \left( \frac{x_i}{x_0} \right)^{-\theta} \\
        &= \theta^{\alpha + n - 1}  e^{-\lambda \theta}
          \prod_{i = 1}^n
          \left( \frac{x_i}{x_0} \right)^{-\theta} \\
        &= \theta^{\alpha + n - 1}  e^{-\lambda \theta}
          e^{-\theta \sum_{i = 1}^n \ln(x_i/x_0)} \\
        &= \theta^{\alpha + n - 1}
          \exp\left\{ -\theta \left( \lambda + \sum_{i = 1}^n \ln
          \left( \frac{x_i}{x_0} \right) \right) \right\},
      \end{align*}
      d'où nous identifions que la distribution à postériori de
      $\Theta$ est gamma de paramètres
      \begin{align*}
        \tilde{\alpha}
        &= \alpha + n \\
        \tilde{\lambda}
        &= \lambda + \sum_{i = 1}^n \ln \left( \frac{x_i}{x_0} \right).
      \end{align*}
    \item L'estimateur à priori de $\theta$ (ou la «prime collective») est
      \begin{equation*}
        \esp{\Theta} = \frac{\alpha}{\lambda}.
      \end{equation*}
      Dans la mesure où la distribution à postériori est du même type
      que la distribution à priori, nous avons directement que
      l'estimateur bayésien de $\theta$ est
      \begin{align*}
        \hat{\Theta}
        &= \frac{\tilde{\alpha}}{\tilde{\lambda}} \\
        &= \frac{\alpha + n}{\lambda + \sum_{i = 1}^n \ln (x_i/x_0)} \\
        &= \eta \hat{\theta}^{\text{EMV}} +
          (1 - \eta) \frac{\alpha}{\lambda} \\
        \intertext{avec}
        \eta
        &= \frac{\sum_{i = 1}^n \ln (x_i/x_0)}{\lambda + \sum_{i = 1}^n \ln (x_i/x_0)}.
      \end{align*}
      Contrairement aux cas de crédibilité bayésienne linéaire, le
      facteur $\eta$ dépend des observations et il ne se trouve pas
      confiné à l'intervalle $(0, 1)$. Il ne s'agit donc pas d'un
      facteur de crédibilité. Par conséquent, $\hat{\Theta}$ ne
      constitue pas une prime de crédibilité. Une seule des deux
      conditions de \cite{Jewell:exact:1974} est vérifiée dans le cas
      présent.
    \end{enumerate}
  \end{sol}
\end{exercice}

\Closesolutionfile{solutions}
\Closesolutionfile{reponses}

%%% Local Variables:
%%% TeX-master: "theorie-credibilite-avec-r"
%%% TeX-engine: xetex
%%% coding: utf-8
%%% End:
